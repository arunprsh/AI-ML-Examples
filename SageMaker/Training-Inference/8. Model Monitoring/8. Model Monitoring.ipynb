{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Churn Prediction + Model Monitor\n",
    "This notebook shows how to:\n",
    "* Host a machine learning model in Amazon SageMaker and capture inference requests, results, and metadata \n",
    "* Analyze a training dataset to generate baseline constraints\n",
    "* Monitor a live endpoint for violations against constraints\n",
    "\n",
    "---\n",
    "## Background\n",
    "\n",
    "Amazon SageMaker provides every developer and data scientist with the ability to build, train, and deploy machine learning models quickly. Amazon SageMaker is a fully-managed service that encompasses the entire machine learning workflow. You can label and prepare your data, choose an algorithm, train a model, and then tune and optimize it for deployment. You can deploy your models to production with Amazon SageMaker to make predictions and lower costs than was previously possible.\n",
    "\n",
    "In addition, Amazon SageMaker enables you to capture the input, output and metadata for invocations of the models that you deploy. It also enables you to analyze the data and monitor its quality. In this notebook, you learn how Amazon SageMaker enables these capabilities.\n",
    "\n",
    "\n",
    "![model-monitor](./model-monitor.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role, session\n",
    "from sagemaker.model import Model\n",
    "import sagemaker\n",
    "import boto3\n",
    "import json\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Essentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "prefix = 'classifier/model-monitor'\n",
    "# COPY the training job name of the previous training job you ran from the SageMaker Console under Training Jobs\n",
    "# Let's re-use the Model we trained in Notebook 2 (Built-in Algorithm XGBoost)\n",
    "training_job_name = 'classifier-2020-11-06-01-17-06-402' # CHANGE THIS\n",
    "# COPY S3 model artifact location from the console\n",
    "model_url = 's3://sagemaker-demo-892313895307/clf/model-artifacts/classifier-2020-11-06-01-17-06-402/output/model.tar.gz'\n",
    "container_image_uri = '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:1.0-1-cpu-py3'\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3 Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capture path: s3://sagemaker-us-east-1-892313895307/classifier/model-monitor/datacapture\n",
      "Report path: s3://sagemaker-us-east-1-892313895307/classifier/model-monitor/reports\n"
     ]
    }
   ],
   "source": [
    "data_capture_prefix = '{}/datacapture'.format(prefix)\n",
    "s3_capture_upload_path = 's3://{}/{}'.format(bucket, data_capture_prefix)\n",
    "\n",
    "reports_prefix = '{}/reports'.format(prefix)\n",
    "s3_report_path = 's3://{}/{}'.format(bucket,reports_prefix)\n",
    "\n",
    "code_prefix = '{}/code'.format(prefix)\n",
    "\n",
    "print(\"Capture path: {}\".format(s3_capture_upload_path))\n",
    "print(\"Report path: {}\".format(s3_report_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capture Real-Time Inference Data from SageMaker Endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "model = Model(image=container_image_uri, model_data=model_url, role=role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-deploy using DataCaptureConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enable data capture for monitoring the model data quality, you specify the new capture option called DataCaptureConfig. You can capture the request payload, the response payload or both with this configuration. The capture config applies to all variants. Go ahead with the deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "from time import gmtime, strftime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EndpointName=classifier-xgboost-model-monitor-2020-11-08-01-54-47\n"
     ]
    }
   ],
   "source": [
    "endpoint_name = 'classifier-xgboost-model-monitor-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(\"EndpointName={}\".format(endpoint_name))\n",
    "\n",
    "data_capture_config = DataCaptureConfig(\n",
    "                        enable_capture=True,\n",
    "                        sampling_percentage=100,\n",
    "                        destination_s3_uri=s3_capture_upload_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'enable_capture': True,\n",
       " 'sampling_percentage': 100,\n",
       " 'destination_s3_uri': 's3://sagemaker-us-east-1-892313895307/classifier/model-monitor/datacapture',\n",
       " 'kms_key_id': None,\n",
       " 'capture_options': ['REQUEST', 'RESPONSE'],\n",
       " 'csv_content_types': ['text/csv'],\n",
       " 'json_content_types': ['application/json']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_capture_config.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "model.deploy(initial_instance_count=1, \n",
    "             instance_type='ml.m5.xlarge', \n",
    "             endpoint_name=endpoint_name, \n",
    "             data_capture_config=data_capture_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke the Deployed Model Monitor Endpoint \n",
    "\n",
    "\n",
    "You can now send data to this endpoint to get inferences in real time. Because you enabled the data capture in the previous steps, the request and response payload, along with some additional metadata, is saved in the Amazon Simple Storage Service (Amazon S3) location you have specified in the DataCaptureConfig."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step invokes the endpoint with included sample data for about 2 minutes. Data is captured based on the sampling percentage specified and the capture continues until the data capture option is turned off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:5: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.predictor import RealTimePredictor\n",
    "from sagemaker.predictor import csv_serializer\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = RealTimePredictor(endpoint=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.content_type = 'text/csv'\n",
    "predictor.serializer = csv_serializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('.././DATA/test/test.csv', names=['class', 'mass', 'width', 'height', 'color_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>mass</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>color_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.382353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.371429</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.646154</td>\n",
       "      <td>0.588235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.314286</td>\n",
       "      <td>0.441176</td>\n",
       "      <td>0.569231</td>\n",
       "      <td>0.323529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.157143</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.676923</td>\n",
       "      <td>0.441176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0.457143</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.529412</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class      mass     width    height  color_score\n",
       "0  1      0.142857  0.058824  0.538462  0.382353   \n",
       "1  3      0.371429  0.529412  0.646154  0.588235   \n",
       "2  0      0.314286  0.441176  0.569231  0.323529   \n",
       "3  1      0.157143  0.058824  0.676923  0.441176   \n",
       "4  3      0.457143  0.500000  0.800000  0.529412   "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "3.0\n",
      "1.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "1.0\n",
      "3.0\n",
      "0.0\n",
      "0.0\n",
      "3.0\n",
      "3.0\n",
      "0.0\n",
      "0.0\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "for _, row in test_df.iterrows():\n",
    "    X =[row.mass, row.width, row.height, row.color_score]\n",
    "    payload = np.array(X)\n",
    "    response = predictor.predict(data=payload)\n",
    "    print(response.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View captured data\n",
    "\n",
    "NOTE: Could take a minute here\n",
    "\n",
    "Now list the data capture files stored in Amazon S3. You should expect to see different files from different time periods organized based on the hour in which the invocation occurred. The format of the Amazon S3 path is:\n",
    "\n",
    "`s3://{destination-bucket-prefix}/{endpoint-name}/{variant-name}/yyyy/mm/dd/hh/filename.jsonl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'classifier/model-monitor/datacapture/classifier-xgboost-model-monitor-2020-11-08-01-54-47'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_client = boto3.Session().client('s3')\n",
    "current_endpoint_capture_prefix = '{}/{}'.format(data_capture_prefix, endpoint_name)\n",
    "current_endpoint_capture_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Key': 'classifier/model-monitor/datacapture/classifier-xgboost-model-monitor-2020-11-08-01-54-47/AllTraffic/2020/11/08/02/01-24-794-3a684b44-17c4-4003-bfbe-caacf1be0120.jsonl',\n",
       "  'LastModified': datetime.datetime(2020, 11, 8, 2, 2, 29, tzinfo=tzlocal()),\n",
       "  'ETag': '\"103515d10cc84349a6562fe59f26409a\"',\n",
       "  'Size': 6326,\n",
       "  'StorageClass': 'STANDARD'}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = s3_client.list_objects(Bucket=bucket, Prefix=current_endpoint_capture_prefix)\n",
    "result.get('Contents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Capture Files:\n",
      "classifier/model-monitor/datacapture/classifier-xgboost-model-monitor-2020-11-08-01-54-47/AllTraffic/2020/11/08/02/01-24-794-3a684b44-17c4-4003-bfbe-caacf1be0120.jsonl\n"
     ]
    }
   ],
   "source": [
    "capture_files = [capture_file.get(\"Key\") for capture_file in result.get('Contents')]\n",
    "print(\"Found Capture Files:\")\n",
    "print(\"\\n \".join(capture_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, view the contents of a single capture file. Here you should see all the data captured in an Amazon SageMaker specific JSON-line formatted file. Take a quick peek at the first few lines in the captured file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_obj_body(obj_key):\n",
    "    return s3_client.get_object(Bucket=bucket, Key=obj_key).get('Body').read().decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"captureData\":{\"endpointInput\":{\"observedContentType\":\"text/csv\",\"mode\":\"INPUT\",\"data\":\"0.14285714285714285,0.058823529411764726,0.5384615384615385,0.38235294117647056\",\"encoding\":\"CSV\"},\"endpointOutput\":{\"observedContentType\":\"text/csv; charset=utf-8\",\"mode\":\"OUTPUT\",\"data\":\"1.0\",\"encoding\":\"CSV\"}},\"eventMetadata\":{\"eventId\":\"cf0945fc-0971-470a-9903-639f1122781c\",\"inferenceTime\":\"2020-11-08T02:01:24Z\"},\"eventVersion\":\"0\"}\\n{\"captureData\":{\"endpointInput\":{\"observedContentType\":\"text/csv\",\"mode\":\"INPUT\",\"data\":\"0.3714285714285714,0.5294117647058825,0.6461538461538461,0.5882352941176472\",\"encoding\":\"CSV\"},\"endpointOutput\":{\"observedContentType\":\"text/csv; charset=utf-8\",\"mode\":\"OUTPUT\",\"data\":\"3.0\",\"encoding\":\"CSV\"}},\"eventMetadata\":{\"eventId\":\"1960cc16-c0bd-494d-bc9f-f6b602b8947d\",\"inferenceTime\":\"2020-11-08T02:01:24Z\"},\"eventVersion\":\"0\"}\\n{\"captureData\":{\"endpointInput\":{\"observedContentType\":\"text/csv\",\"mode\":\"INPUT\",\"data\":\"0.31428571428571433,0.4411764705882353,0.5692307692307692,0'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "capture_file = get_obj_body(capture_files[-1])\n",
    "capture_file[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the contents of a single line is present below in a formatted JSON file so that you can observe a little better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"captureData\": {\n",
      "    \"endpointInput\": {\n",
      "      \"observedContentType\": \"text/csv\",\n",
      "      \"mode\": \"INPUT\",\n",
      "      \"data\": \"0.14285714285714285,0.058823529411764726,0.5384615384615385,0.38235294117647056\",\n",
      "      \"encoding\": \"CSV\"\n",
      "    },\n",
      "    \"endpointOutput\": {\n",
      "      \"observedContentType\": \"text/csv; charset=utf-8\",\n",
      "      \"mode\": \"OUTPUT\",\n",
      "      \"data\": \"1.0\",\n",
      "      \"encoding\": \"CSV\"\n",
      "    }\n",
      "  },\n",
      "  \"eventMetadata\": {\n",
      "    \"eventId\": \"cf0945fc-0971-470a-9903-639f1122781c\",\n",
      "    \"inferenceTime\": \"2020-11-08T02:01:24Z\"\n",
      "  },\n",
      "  \"eventVersion\": \"0\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(json.loads(capture_file.split('\\n')[0]), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each inference request is captured in one line in the jsonl file. The line contains both the input and output merged together. In the example, you provided the ContentType as `text/csv` which is reflected in the `observedContentType` value. Also, you expose the encoding that you used to encode the input and output payloads in the capture format with the `encoding` value.\n",
    "\n",
    "To recap, you observed how you can enable capturing the input or output payloads to an endpoint with a new parameter. You have also observed what the captured format looks like in Amazon S3. Next, continue to explore how Amazon SageMaker helps with monitoring the data collected in Amazon S3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseling & Continuous Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to collecting the data, Amazon SageMaker provides the capability for you to monitor and evaluate the data observed by the endpoints. For this:\n",
    "1. Create a baseline with which you compare the realtime traffic. \n",
    "1. Once a baseline is ready, setup a schedule to continously evaluate and compare against the baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training dataset with which you trained the model is usually a good baseline dataset. Note that the training dataset data schema and the inference dataset schema should exactly match (i.e. the number and order of the features).\n",
    "\n",
    "From the training dataset you can ask Amazon SageMaker to suggest a set of baseline `constraints` and generate descriptive `statistics` to explore the data. For this example, upload the training dataset that was used to train the pre-trained model included in this example. If you already have it in Amazon S3, you can directly point to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline data uri: s3://sagemaker-us-east-1-892313895307/classifier/model-monitor/baselining/data\n",
      "Baseline results uri: s3://sagemaker-us-east-1-892313895307/classifier/model-monitor/baselining/results\n"
     ]
    }
   ],
   "source": [
    "# copy over the training dataset to Amazon S3 (if you already have it in Amazon S3, you could reuse it)\n",
    "baseline_prefix = prefix + '/baselining'\n",
    "baseline_data_prefix = baseline_prefix + '/data'\n",
    "baseline_results_prefix = baseline_prefix + '/results'\n",
    "\n",
    "baseline_data_uri = 's3://{}/{}'.format(bucket,baseline_data_prefix)\n",
    "baseline_results_uri = 's3://{}/{}'.format(bucket, baseline_results_prefix)\n",
    "print('Baseline data uri: {}'.format(baseline_data_uri))\n",
    "print('Baseline results uri: {}'.format(baseline_results_uri))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Baselining Job with Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "from sagemaker.model_monitor import DefaultModelMonitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload Train Set to S3 as Baseline Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = open('.././DATA/train/train_with_header.csv', 'rb')\n",
    "s3_key = os.path.join(baseline_prefix, 'data', 'train_with_header.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(s3_key).upload_fileobj(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_model_monitor = DefaultModelMonitor(\n",
    "                            role=role,\n",
    "                            instance_count=1,\n",
    "                            instance_type='ml.r5.xlarge',\n",
    "                            volume_size_in_gb=20,\n",
    "                            max_runtime_in_seconds=3600,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  baseline-suggestion-job-2020-11-08-02-19-12-931\n",
      "Inputs:  [{'InputName': 'baseline_dataset_input', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-892313895307/classifier/model-monitor/baselining/data/train_with_header.csv', 'LocalPath': '/opt/ml/processing/input/baseline_dataset_input', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'monitoring_output', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-892313895307/classifier/model-monitor/baselining/results', 'LocalPath': '/opt/ml/processing/output', 'S3UploadMode': 'EndOfJob'}}]\n",
      "........................\u001b[34m2020-11-08 02:23:01,577 - __main__ - INFO - All params:{'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:892313895307:processing-job/baseline-suggestion-job-2020-11-08-02-19-12-931', 'ProcessingJobName': 'baseline-suggestion-job-2020-11-08-02-19-12-931', 'Environment': {'dataset_format': '{\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}, 'AppSpecification': {'ImageUri': '156813124566.dkr.ecr.us-east-1.amazonaws.com/sagemaker-model-monitor-analyzer', 'ContainerEntrypoint': None, 'ContainerArguments': None}, 'ProcessingInputs': [{'InputName': 'baseline_dataset_input', 'S3Input': {'LocalPath': '/opt/ml/processing/input/baseline_dataset_input', 'S3Uri': 's3://sagemaker-us-east-1-892313895307/classifier/model-monitor/baselining/data/train_with_header.csv', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'monitoring_output', 'S3Output': {'LocalPath': '/opt/ml/processing/output', 'S3Uri': 's3://sagemaker-us-east-1-892313895307/classifier/model-monitor/baselining/results', 'S3UploadMode': 'EndOfJob'}, 'FeatureStoreOutput': None}], 'KmsKeyId': None}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1, 'InstanceType': 'ml.r5.xlarge', 'VolumeSizeInGB': 20, 'VolumeKmsKeyId': None}}, 'RoleArn': 'arn:aws:iam::892313895307:role/service-role/AmazonSageMaker-ExecutionRole-20200827T161464', 'StoppingCondition': {'MaxRuntimeInSeconds': 3600}}\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:01,577 - __main__ - INFO - Current Environment:{'dataset_format': '{\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:01,577 - DefaultDataAnalyzer - INFO - Performing analysis with input: {\"dataset_source\": \"/opt/ml/processing/input/baseline_dataset_input\", \"dataset_format\": {\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}, \"record_preprocessor_script\": null, \"post_analytics_processor_script\": null, \"baseline_constraints\": null, \"baseline_statistics\": null, \"output_path\": \"/opt/ml/processing/output\", \"start_time\": null, \"end_time\": null, \"cloudwatch_metrics_directory\": \"/opt/ml/output/metrics/cloudwatch\", \"publish_cloudwatch_metrics\": \"Disabled\", \"sagemaker_endpoint_name\": null, \"sagemaker_monitoring_schedule_name\": null, \"output_message_file\": \"/opt/ml/output/message\"}\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:01,577 - DefaultDataAnalyzer - INFO - Bootstrapping yarn\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:01,577 - bootstrap - INFO - Copy aws jars\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:01,634 - bootstrap - INFO - Copy cluster config\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:01,635 - bootstrap - INFO - Write runtime cluster config\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:01,635 - bootstrap - INFO - Resource Config is: {'current_host': 'algo-1', 'hosts': ['algo-1']}\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:01,644 - bootstrap - INFO - Finished Yarn configuration files setup.\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:01,645 - bootstrap - INFO - Starting spark process for master node algo-1\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:01,645 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs namenode -format -force\u001b[0m\n",
      "\u001b[34mWARNING: /usr/hadoop-3.0.0/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,117 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.122.144\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = [-format, -force]\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.0.0\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/hadoop-3.0.0/etc/hadoop:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/junit-4.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/aws-java-sdk-bundle-1.11.199.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-aws-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-kms-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okio-1.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-annotations-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-runtime-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-prefix-tree-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-api-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/htrace-core-3.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-procedure-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/servlet-api-2.5-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop2-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-protocol-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-csv-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-client-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jcodings-1.0.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-compiler-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-math-2.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jamon-runtime-2.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-2.2.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/joni-2.1.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-common-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/findbugs-annotations-1.3.9-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-el-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/disruptor-3.3.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/fst-2.50.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-server-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-api-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-router-3.0.0.jar:/usr/ha\u001b[0m\n",
      "\u001b[34mdoop-3.0.0/share/hadoop/yarn/hadoop-yarn-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-registry-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-3.0.0.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r c25427ceca461ee979d30edd7a4b0f50718e6533; compiled by 'andrew' on 2017-12-08T19:16Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_265\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,127 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,131 INFO namenode.NameNode: createNameNode [-format, -force]\u001b[0m\n",
      "\u001b[34mFormatting using clusterid: CID-bcf7b922-586e-408c-9806-6a11d3c372f3\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,615 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,630 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,631 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,634 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,638 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,639 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,639 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,639 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,675 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,688 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,688 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,696 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,696 INFO blockmanagement.BlockManager: The block deletion will start around 2020 Nov 08 02:23:02\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,697 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,697 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,699 INFO util.GSet: 2.0% max memory 6.7 GB = 136.4 MB\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,699 INFO util.GSet: capacity      = 2^24 = 16777216 entries\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,752 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,757 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,757 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,757 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,757 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,757 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,757 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,757 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,757 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,757 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,757 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,757 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,788 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,788 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,788 INFO util.GSet: 1.0% max memory 6.7 GB = 68.2 MB\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,788 INFO util.GSet: capacity      = 2^23 = 8388608 entries\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,805 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,805 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,805 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,805 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,810 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,813 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,813 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,814 INFO util.GSet: 0.25% max memory 6.7 GB = 17.0 MB\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,814 INFO util.GSet: capacity      = 2^21 = 2097152 entries\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,820 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,820 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,820 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,824 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,824 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,825 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,825 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,825 INFO util.GSet: 0.029999999329447746% max memory 6.7 GB = 2.0 MB\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,825 INFO util.GSet: capacity      = 2^18 = 262144 entries\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,846 INFO namenode.FSImage: Allocated new BlockPoolId: BP-880288279-10.0.122.144-1604802182841\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,860 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,867 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,948 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 385 bytes saved in 0 seconds.\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,960 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,963 INFO namenode.NameNode: SHUTDOWN_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSHUTDOWN_MSG: Shutting down NameNode at algo-1/10.0.122.144\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:02,975 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:05,022 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode, return code 1\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:05,023 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:07,082 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode, return code 1\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:07,082 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:09,161 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager, return code 1\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:09,161 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:11,272 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager, return code 1\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:11,273 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2020-11-08 02:23:13,394 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver, return code 1\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:13,394 - DefaultDataAnalyzer - INFO - Total number of hosts in the cluster: 1\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:23,404 - DefaultDataAnalyzer - INFO - Running command: bin/spark-submit --master yarn --deploy-mode client --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider --conf spark.serializer=org.apache.spark.serializer.KryoSerializer /opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:24 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:24 INFO  Main:27 - Start analyzing with args: --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:24 INFO  Main:30 - Analytics input path: DataAnalyzerParams(/tmp/spark_job_config.json,yarn)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:24 INFO  FileUtil:66 - Read file from path /tmp/spark_job_config.json.\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:25 INFO  SparkContext:54 - Running Spark version 2.3.1\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:25 INFO  SparkContext:54 - Submitted application: SageMakerDataAnalyzer\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:25 INFO  SecurityManager:54 - Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:25 INFO  SecurityManager:54 - Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:25 INFO  SecurityManager:54 - Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:25 INFO  SecurityManager:54 - Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:25 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:25 INFO  Utils:54 - Successfully started service 'sparkDriver' on port 40267.\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:25 INFO  SparkEnv:54 - Registering MapOutputTracker\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:25 INFO  SparkEnv:54 - Registering BlockManagerMaster\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:25 INFO  BlockManagerMasterEndpoint:54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:25 INFO  BlockManagerMasterEndpoint:54 - BlockManagerMasterEndpoint up\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:25 INFO  DiskBlockManager:54 - Created local directory at /tmp/blockmgr-9bf8211e-7178-48ab-9227-c82c8b6bdbe9\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:25 INFO  MemoryStore:54 - MemoryStore started with capacity 1458.6 MB\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:25 INFO  SparkEnv:54 - Registering OutputCommitCoordinator\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:25 INFO  SparkContext:54 - Added JAR file:/opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar at spark://10.0.122.144:40267/jars/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar with timestamp 1604802205411\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:26 INFO  RMProxy:133 - Connecting to ResourceManager at /10.0.122.144:8032\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:26 INFO  Client:54 - Requesting a new application from cluster with 1 NodeManagers\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:26 INFO  Configuration:2636 - resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:26 INFO  ResourceUtils:427 - Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:26 INFO  Client:54 - Verifying our application has not requested more than the maximum memory capability of the cluster (31706 MB per container)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:26 INFO  Client:54 - Will allocate AM container, with 896 MB memory including 384 MB overhead\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:26 INFO  Client:54 - Setting up container launch context for our AM\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:26 INFO  Client:54 - Setting up the launch environment for our AM container\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:26 INFO  Client:54 - Preparing resources for our AM container\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:27 WARN  Client:66 - Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:28 INFO  Client:54 - Uploading resource file:/tmp/spark-cd213f05-d020-4061-93c0-a0e02639e254/__spark_libs__5404637255064357502.zip -> hdfs://10.0.122.144/user/root/.sparkStaging/application_1604802188580_0001/__spark_libs__5404637255064357502.zip\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:30 INFO  Client:54 - Uploading resource file:/tmp/spark-cd213f05-d020-4061-93c0-a0e02639e254/__spark_conf__527732477619983078.zip -> hdfs://10.0.122.144/user/root/.sparkStaging/application_1604802188580_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:30 INFO  SecurityManager:54 - Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:30 INFO  SecurityManager:54 - Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:30 INFO  SecurityManager:54 - Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:30 INFO  SecurityManager:54 - Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:30 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:30 INFO  Client:54 - Submitting application application_1604802188580_0001 to ResourceManager\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:30 INFO  YarnClientImpl:310 - Submitted application application_1604802188580_0001\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:30 INFO  SchedulerExtensionServices:54 - Starting Yarn extension services with app application_1604802188580_0001 and attemptId None\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:31 INFO  Client:54 - Application report for application_1604802188580_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:31 INFO  Client:54 - \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: AM container is launched, waiting for AM container to Register with RM\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1604802210410\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1604802188580_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:32 INFO  Client:54 - Application report for application_1604802188580_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:33 INFO  Client:54 - Application report for application_1604802188580_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:34 INFO  Client:54 - Application report for application_1604802188580_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:34 INFO  YarnClientSchedulerBackend:54 - Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1604802188580_0001), /proxy/application_1604802188580_0001\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:35 INFO  YarnSchedulerBackend$YarnSchedulerEndpoint:54 - ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:35 INFO  Client:54 - Application report for application_1604802188580_0001 (state: RUNNING)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:35 INFO  Client:54 - \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: 10.0.122.144\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: 0\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1604802210410\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1604802188580_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:35 INFO  YarnClientSchedulerBackend:54 - Application application_1604802188580_0001 has started running.\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:35 INFO  Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40967.\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:35 INFO  NettyBlockTransferService:54 - Server created on 10.0.122.144:40967\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:35 INFO  BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:35 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, 10.0.122.144, 40967, None)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:35 INFO  BlockManagerMasterEndpoint:54 - Registering block manager 10.0.122.144:40967 with 1458.6 MB RAM, BlockManagerId(driver, 10.0.122.144, 40967, None)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:35 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, 10.0.122.144, 40967, None)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:35 INFO  BlockManager:54 - Initialized BlockManager: BlockManagerId(driver, 10.0.122.144, 40967, None)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:35 INFO  log:192 - Logging initialized @12076ms\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:37 INFO  YarnSchedulerBackend$YarnDriverEndpoint:54 - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.122.144:57610) with ID 1\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:37 INFO  BlockManagerMasterEndpoint:54 - Registering block manager algo-1:43055 with 11.9 GB RAM, BlockManagerId(1, algo-1, 43055, None)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2020-11-08 02:23:55 INFO  YarnClientSchedulerBackend:54 - SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000(ms)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:55 WARN  SparkContext:66 - Spark is not running in local mode, therefore the checkpoint directory must not be on the local filesystem. Directory '/tmp' appears to be on the local filesystem.\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:55 INFO  DatasetReader:90 - Files to process:List(file:///opt/ml/processing/input/baseline_dataset_input/train_with_header.csv)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:55 INFO  SharedState:54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/usr/spark-2.3.1/spark-warehouse').\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:55 INFO  SharedState:54 - Warehouse path is 'file:/usr/spark-2.3.1/spark-warehouse'.\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:55 INFO  StateStoreCoordinatorRef:54 - Registered StateStoreCoordinator endpoint\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:56 INFO  FileSourceStrategy:54 - Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:56 INFO  FileSourceStrategy:54 - Post-Scan Filters: (length(trim(value#0, None)) > 0)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:56 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:56 INFO  FileSourceScanExec:54 - Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:57 INFO  CodeGenerator:54 - Code generated in 174.55158 ms\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:57 INFO  CodeGenerator:54 - Code generated in 28.326984 ms\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:57 INFO  MemoryStore:54 - Block broadcast_0 stored as values in memory (estimated size 429.5 KB, free 1458.2 MB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:57 INFO  MemoryStore:54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 38.2 KB, free 1458.1 MB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:57 INFO  BlockManagerInfo:54 - Added broadcast_0_piece0 in memory on 10.0.122.144:40967 (size: 38.2 KB, free: 1458.6 MB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:57 INFO  SparkContext:54 - Created broadcast 0 from csv at DatasetReader.scala:49\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:57 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:57 INFO  SparkContext:54 - Starting job: csv at DatasetReader.scala:49\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:57 INFO  DAGScheduler:54 - Got job 0 (csv at DatasetReader.scala:49) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:57 INFO  DAGScheduler:54 - Final stage: ResultStage 0 (csv at DatasetReader.scala:49)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:57 INFO  DAGScheduler:54 - Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:57 INFO  DAGScheduler:54 - Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:57 INFO  DAGScheduler:54 - Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at DatasetReader.scala:49), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:57 INFO  MemoryStore:54 - Block broadcast_1 stored as values in memory (estimated size 9.4 KB, free 1458.1 MB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:57 INFO  MemoryStore:54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.5 KB, free 1458.1 MB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:57 INFO  BlockManagerInfo:54 - Added broadcast_1_piece0 in memory on 10.0.122.144:40967 (size: 4.5 KB, free: 1458.6 MB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:57 INFO  SparkContext:54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1039\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:57 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at DatasetReader.scala:49) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:57 INFO  YarnScheduler:54 - Adding task set 0.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:57 INFO  TaskSetManager:54 - Starting task 0.0 in stage 0.0 (TID 0, algo-1, executor 1, partition 0, PROCESS_LOCAL, 8352 bytes)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:58 INFO  BlockManagerInfo:54 - Added broadcast_1_piece0 in memory on algo-1:43055 (size: 4.5 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:59 INFO  BlockManagerInfo:54 - Added broadcast_0_piece0 in memory on algo-1:43055 (size: 38.2 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:59 INFO  TaskSetManager:54 - Finished task 0.0 in stage 0.0 (TID 0) in 1704 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:59 INFO  YarnScheduler:54 - Removed TaskSet 0.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:59 INFO  DAGScheduler:54 - ResultStage 0 (csv at DatasetReader.scala:49) finished in 1.777 s\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:59 INFO  DAGScheduler:54 - Job 0 finished: csv at DatasetReader.scala:49, took 1.823598 s\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:59 INFO  FileSourceStrategy:54 - Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:59 INFO  FileSourceStrategy:54 - Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:59 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:59 INFO  FileSourceScanExec:54 - Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:59 INFO  CodeGenerator:54 - Code generated in 8.50447 ms\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:59 INFO  MemoryStore:54 - Block broadcast_2 stored as values in memory (estimated size 429.5 KB, free 1457.7 MB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:59 INFO  MemoryStore:54 - Block broadcast_2_piece0 stored as bytes in memory (estimated size 38.2 KB, free 1457.7 MB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:59 INFO  BlockManagerInfo:54 - Added broadcast_2_piece0 in memory on 10.0.122.144:40967 (size: 38.2 KB, free: 1458.5 MB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:59 INFO  SparkContext:54 - Created broadcast 2 from csv at DatasetReader.scala:49\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:59 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:59 INFO  FileSourceStrategy:54 - Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:59 INFO  FileSourceStrategy:54 - Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:59 INFO  FileSourceStrategy:54 - Output Data Schema: struct<mass: string, width: string, height: string, color_score: string ... 2 more fields>\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:59 INFO  FileSourceScanExec:54 - Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:59 INFO  MemoryStore:54 - Block broadcast_3 stored as values in memory (estimated size 429.5 KB, free 1457.3 MB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:59 INFO  MemoryStore:54 - Block broadcast_3_piece0 stored as bytes in memory (estimated size 38.2 KB, free 1457.2 MB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:59 INFO  BlockManagerInfo:54 - Added broadcast_3_piece0 in memory on 10.0.122.144:40967 (size: 38.2 KB, free: 1458.5 MB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:59 INFO  SparkContext:54 - Created broadcast 3 from cache at DataAnalyzer.scala:76\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:59 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:59 INFO  CodeGenerator:54 - Code generated in 15.398913 ms\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:59 INFO  CodeGenerator:54 - Code generated in 14.257382 ms\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:59 INFO  SparkContext:54 - Starting job: head at DataAnalyzer.scala:79\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:59 INFO  DAGScheduler:54 - Got job 1 (head at DataAnalyzer.scala:79) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:59 INFO  DAGScheduler:54 - Final stage: ResultStage 1 (head at DataAnalyzer.scala:79)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:59 INFO  DAGScheduler:54 - Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:59 INFO  DAGScheduler:54 - Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:59 INFO  DAGScheduler:54 - Submitting ResultStage 1 (MapPartitionsRDD[17] at head at DataAnalyzer.scala:79), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:59 INFO  MemoryStore:54 - Block broadcast_4 stored as values in memory (estimated size 19.6 KB, free 1457.2 MB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:59 INFO  MemoryStore:54 - Block broadcast_4_piece0 stored as bytes in memory (estimated size 8.9 KB, free 1457.2 MB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:59 INFO  BlockManagerInfo:54 - Added broadcast_4_piece0 in memory on 10.0.122.144:40967 (size: 8.9 KB, free: 1458.5 MB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:59 INFO  SparkContext:54 - Created broadcast 4 from broadcast at DAGScheduler.scala:1039\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:59 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[17] at head at DataAnalyzer.scala:79) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:59 INFO  YarnScheduler:54 - Adding task set 1.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:59 INFO  TaskSetManager:54 - Starting task 0.0 in stage 1.0 (TID 1, algo-1, executor 1, partition 0, PROCESS_LOCAL, 8352 bytes)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:23:59 INFO  BlockManagerInfo:54 - Added broadcast_4_piece0 in memory on algo-1:43055 (size: 8.9 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:00 INFO  BlockManagerInfo:54 - Added broadcast_3_piece0 in memory on algo-1:43055 (size: 38.2 KB, free: 11.9 GB)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m2020-11-08 02:24:00 INFO  BlockManagerInfo:54 - Added rdd_11_0 in memory on algo-1:43055 (size: 3.5 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:00 INFO  TaskSetManager:54 - Finished task 0.0 in stage 1.0 (TID 1) in 371 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:00 INFO  DAGScheduler:54 - ResultStage 1 (head at DataAnalyzer.scala:79) finished in 0.408 s\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:00 INFO  YarnScheduler:54 - Removed TaskSet 1.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:00 INFO  DAGScheduler:54 - Job 1 finished: head at DataAnalyzer.scala:79, took 0.413118 s\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:00 WARN  Utils:66 - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:00 INFO  CodeGenerator:54 - Code generated in 18.337049 ms\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:00 INFO  SparkContext:54 - Starting job: collect at AnalysisRunner.scala:313\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:00 INFO  DAGScheduler:54 - Registering RDD 22 (collect at AnalysisRunner.scala:313)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:00 INFO  DAGScheduler:54 - Got job 2 (collect at AnalysisRunner.scala:313) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:00 INFO  DAGScheduler:54 - Final stage: ResultStage 3 (collect at AnalysisRunner.scala:313)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:00 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 2)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:00 INFO  DAGScheduler:54 - Missing parents: List(ShuffleMapStage 2)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:00 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 2 (MapPartitionsRDD[22] at collect at AnalysisRunner.scala:313), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:00 INFO  MemoryStore:54 - Block broadcast_5 stored as values in memory (estimated size 56.3 KB, free 1457.1 MB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:00 INFO  MemoryStore:54 - Block broadcast_5_piece0 stored as bytes in memory (estimated size 20.3 KB, free 1457.1 MB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:00 INFO  BlockManagerInfo:54 - Added broadcast_5_piece0 in memory on 10.0.122.144:40967 (size: 20.3 KB, free: 1458.5 MB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:00 INFO  SparkContext:54 - Created broadcast 5 from broadcast at DAGScheduler.scala:1039\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:00 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[22] at collect at AnalysisRunner.scala:313) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:00 INFO  YarnScheduler:54 - Adding task set 2.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:00 INFO  TaskSetManager:54 - Starting task 0.0 in stage 2.0 (TID 2, algo-1, executor 1, partition 0, PROCESS_LOCAL, 8341 bytes)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:00 INFO  BlockManagerInfo:54 - Added broadcast_5_piece0 in memory on algo-1:43055 (size: 20.3 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:01 INFO  TaskSetManager:54 - Finished task 0.0 in stage 2.0 (TID 2) in 819 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:01 INFO  YarnScheduler:54 - Removed TaskSet 2.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:01 INFO  DAGScheduler:54 - ShuffleMapStage 2 (collect at AnalysisRunner.scala:313) finished in 0.839 s\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:01 INFO  DAGScheduler:54 - looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:01 INFO  DAGScheduler:54 - running: Set()\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:01 INFO  DAGScheduler:54 - waiting: Set(ResultStage 3)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:01 INFO  DAGScheduler:54 - failed: Set()\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:01 INFO  DAGScheduler:54 - Submitting ResultStage 3 (MapPartitionsRDD[25] at collect at AnalysisRunner.scala:313), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:01 INFO  MemoryStore:54 - Block broadcast_6 stored as values in memory (estimated size 66.6 KB, free 1457.0 MB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:01 INFO  MemoryStore:54 - Block broadcast_6_piece0 stored as bytes in memory (estimated size 22.5 KB, free 1457.0 MB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:01 INFO  BlockManagerInfo:54 - Added broadcast_6_piece0 in memory on 10.0.122.144:40967 (size: 22.5 KB, free: 1458.4 MB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:01 INFO  SparkContext:54 - Created broadcast 6 from broadcast at DAGScheduler.scala:1039\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:01 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[25] at collect at AnalysisRunner.scala:313) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:01 INFO  YarnScheduler:54 - Adding task set 3.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:01 INFO  TaskSetManager:54 - Starting task 0.0 in stage 3.0 (TID 3, algo-1, executor 1, partition 0, NODE_LOCAL, 7765 bytes)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:01 INFO  BlockManagerInfo:54 - Added broadcast_6_piece0 in memory on algo-1:43055 (size: 22.5 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:01 INFO  MapOutputTrackerMasterEndpoint:54 - Asked to send map output locations for shuffle 0 to 10.0.122.144:57610\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:01 INFO  TaskSetManager:54 - Finished task 0.0 in stage 3.0 (TID 3) in 308 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:01 INFO  DAGScheduler:54 - ResultStage 3 (collect at AnalysisRunner.scala:313) finished in 0.319 s\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:01 INFO  DAGScheduler:54 - Job 2 finished: collect at AnalysisRunner.scala:313, took 1.176000 s\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:01 INFO  YarnScheduler:54 - Removed TaskSet 3.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:02 INFO  CodeGenerator:54 - Code generated in 18.654075 ms\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:02 INFO  SparkContext:54 - Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:02 INFO  DAGScheduler:54 - Got job 3 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:02 INFO  DAGScheduler:54 - Final stage: ResultStage 4 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:02 INFO  DAGScheduler:54 - Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:02 INFO  DAGScheduler:54 - Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:02 INFO  DAGScheduler:54 - Submitting ResultStage 4 (MapPartitionsRDD[34] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:02 INFO  MemoryStore:54 - Block broadcast_7 stored as values in memory (estimated size 23.9 KB, free 1457.0 MB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:02 INFO  MemoryStore:54 - Block broadcast_7_piece0 stored as bytes in memory (estimated size 10.6 KB, free 1457.0 MB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:02 INFO  BlockManagerInfo:54 - Added broadcast_7_piece0 in memory on 10.0.122.144:40967 (size: 10.6 KB, free: 1458.4 MB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:02 INFO  SparkContext:54 - Created broadcast 7 from broadcast at DAGScheduler.scala:1039\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:02 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[34] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:02 INFO  YarnScheduler:54 - Adding task set 4.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:02 INFO  TaskSetManager:54 - Starting task 0.0 in stage 4.0 (TID 4, algo-1, executor 1, partition 0, PROCESS_LOCAL, 8352 bytes)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:02 INFO  BlockManagerInfo:54 - Added broadcast_7_piece0 in memory on algo-1:43055 (size: 10.6 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:02 INFO  TaskSetManager:54 - Finished task 0.0 in stage 4.0 (TID 4) in 181 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:02 INFO  YarnScheduler:54 - Removed TaskSet 4.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:02 INFO  DAGScheduler:54 - ResultStage 4 (treeReduce at KLLRunner.scala:107) finished in 0.198 s\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:02 INFO  DAGScheduler:54 - Job 3 finished: treeReduce at KLLRunner.scala:107, took 0.201956 s\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:02 INFO  CodeGenerator:54 - Code generated in 65.717588 ms\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:02 INFO  CodeGenerator:54 - Code generated in 94.28089 ms\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:02 INFO  CodeGenerator:54 - Code generated in 171.863928 ms\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:02 INFO  ContextCleaner:54 - Cleaned accumulator 44\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:02 INFO  ContextCleaner:54 - Cleaned accumulator 31\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:02 INFO  ContextCleaner:54 - Cleaned accumulator 81\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:02 INFO  ContextCleaner:54 - Cleaned accumulator 152\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:02 INFO  SparkContext:54 - Starting job: collect at AnalysisRunner.scala:313\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:02 INFO  DAGScheduler:54 - Registering RDD 39 (collect at AnalysisRunner.scala:313)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:02 INFO  DAGScheduler:54 - Got job 4 (collect at AnalysisRunner.scala:313) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:02 INFO  DAGScheduler:54 - Final stage: ResultStage 6 (collect at AnalysisRunner.scala:313)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:02 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 5)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:02 INFO  DAGScheduler:54 - Missing parents: List(ShuffleMapStage 5)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:02 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 5 (MapPartitionsRDD[39] at collect at AnalysisRunner.scala:313), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  BlockManagerInfo:54 - Removed broadcast_2_piece0 on 10.0.122.144:40967 in memory (size: 38.2 KB, free: 1458.5 MB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  MemoryStore:54 - Block broadcast_8 stored as values in memory (estimated size 54.2 KB, free 1457.4 MB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  MemoryStore:54 - Block broadcast_8_piece0 stored as bytes in memory (estimated size 19.4 KB, free 1457.4 MB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  BlockManagerInfo:54 - Added broadcast_8_piece0 in memory on 10.0.122.144:40967 (size: 19.4 KB, free: 1458.4 MB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  SparkContext:54 - Created broadcast 8 from broadcast at DAGScheduler.scala:1039\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[39] at collect at AnalysisRunner.scala:313) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  YarnScheduler:54 - Adding task set 5.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  TaskSetManager:54 - Starting task 0.0 in stage 5.0 (TID 5, algo-1, executor 1, partition 0, PROCESS_LOCAL, 8341 bytes)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 88\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 160\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 59\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 118\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 35\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 46\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 100\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 93\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 106\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 104\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 159\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 147\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 19\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 51\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 56\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 83\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 74\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 119\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 109\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 95\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  BlockManagerInfo:54 - Removed broadcast_1_piece0 on algo-1:43055 in memory (size: 4.5 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  BlockManagerInfo:54 - Removed broadcast_1_piece0 on 10.0.122.144:40967 in memory (size: 4.5 KB, free: 1458.4 MB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  BlockManagerInfo:54 - Added broadcast_8_piece0 in memory on algo-1:43055 (size: 19.4 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 24\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  BlockManagerInfo:54 - Removed broadcast_6_piece0 on 10.0.122.144:40967 in memory (size: 22.5 KB, free: 1458.5 MB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  BlockManagerInfo:54 - Removed broadcast_6_piece0 on algo-1:43055 in memory (size: 22.5 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 122\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 127\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 82\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 140\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 14\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 90\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 132\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 145\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 128\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 2\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 76\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 102\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 61\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 50\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 156\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 129\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 130\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 96\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 68\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 115\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 65\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 58\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 71\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 125\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 20\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 15\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 94\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 144\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 7\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned shuffle 0\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 99\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 10\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 63\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 91\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 117\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 143\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 43\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 78\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 45\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 114\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 34\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 131\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 21\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 151\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 22\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 16\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 32\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 84\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  BlockManagerInfo:54 - Removed broadcast_0_piece0 on 10.0.122.144:40967 in memory (size: 38.2 KB, free: 1458.5 MB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  BlockManagerInfo:54 - Removed broadcast_0_piece0 on algo-1:43055 in memory (size: 38.2 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 124\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 0\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 80\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 64\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 62\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 136\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 47\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 29\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 97\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 69\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 13\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 70\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 57\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 111\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 161\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 4\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 148\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 89\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 6\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 116\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 110\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 30\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 112\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 73\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 23\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 54\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 52\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 25\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  BlockManagerInfo:54 - Removed broadcast_7_piece0 on 10.0.122.144:40967 in memory (size: 10.6 KB, free: 1458.5 MB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  BlockManagerInfo:54 - Removed broadcast_7_piece0 on algo-1:43055 in memory (size: 10.6 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  BlockManagerInfo:54 - Removed broadcast_5_piece0 on 10.0.122.144:40967 in memory (size: 20.3 KB, free: 1458.5 MB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  BlockManagerInfo:54 - Removed broadcast_5_piece0 on algo-1:43055 in memory (size: 20.3 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 72\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 155\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 9\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 66\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 138\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 60\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  BlockManagerInfo:54 - Removed broadcast_4_piece0 on 10.0.122.144:40967 in memory (size: 8.9 KB, free: 1458.5 MB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  BlockManagerInfo:54 - Removed broadcast_4_piece0 on algo-1:43055 in memory (size: 8.9 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 67\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 141\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 154\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 92\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 5\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 3\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 1\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 123\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 49\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 158\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 126\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 149\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 153\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 53\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 86\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 18\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 150\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 11\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 87\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 33\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 55\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 137\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 75\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 77\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 157\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 121\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 139\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 85\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 142\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 146\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 27\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 48\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 12\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 98\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 28\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 8\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 17\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 42\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 26\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 101\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 108\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 120\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 103\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 105\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 113\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 107\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  ContextCleaner:54 - Cleaned accumulator 79\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  TaskSetManager:54 - Finished task 0.0 in stage 5.0 (TID 5) in 201 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  YarnScheduler:54 - Removed TaskSet 5.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  DAGScheduler:54 - ShuffleMapStage 5 (collect at AnalysisRunner.scala:313) finished in 0.241 s\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  DAGScheduler:54 - looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  DAGScheduler:54 - running: Set()\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  DAGScheduler:54 - waiting: Set(ResultStage 6)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  DAGScheduler:54 - failed: Set()\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  DAGScheduler:54 - Submitting ResultStage 6 (MapPartitionsRDD[42] at collect at AnalysisRunner.scala:313), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  MemoryStore:54 - Block broadcast_9 stored as values in memory (estimated size 44.4 KB, free 1458.0 MB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  MemoryStore:54 - Block broadcast_9_piece0 stored as bytes in memory (estimated size 13.8 KB, free 1458.0 MB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  BlockManagerInfo:54 - Added broadcast_9_piece0 in memory on 10.0.122.144:40967 (size: 13.8 KB, free: 1458.5 MB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  SparkContext:54 - Created broadcast 9 from broadcast at DAGScheduler.scala:1039\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[42] at collect at AnalysisRunner.scala:313) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  YarnScheduler:54 - Adding task set 6.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  TaskSetManager:54 - Starting task 0.0 in stage 6.0 (TID 6, algo-1, executor 1, partition 0, NODE_LOCAL, 7765 bytes)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  BlockManagerInfo:54 - Added broadcast_9_piece0 in memory on algo-1:43055 (size: 13.8 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  MapOutputTrackerMasterEndpoint:54 - Asked to send map output locations for shuffle 1 to 10.0.122.144:57610\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  TaskSetManager:54 - Finished task 0.0 in stage 6.0 (TID 6) in 77 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  YarnScheduler:54 - Removed TaskSet 6.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  DAGScheduler:54 - ResultStage 6 (collect at AnalysisRunner.scala:313) finished in 0.087 s\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  DAGScheduler:54 - Job 4 finished: collect at AnalysisRunner.scala:313, took 0.390065 s\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  SparkContext:54 - Starting job: countByKey at ColumnProfiler.scala:566\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  DAGScheduler:54 - Registering RDD 49 (countByKey at ColumnProfiler.scala:566)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  DAGScheduler:54 - Got job 5 (countByKey at ColumnProfiler.scala:566) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  DAGScheduler:54 - Final stage: ResultStage 8 (countByKey at ColumnProfiler.scala:566)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 7)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  DAGScheduler:54 - Missing parents: List(ShuffleMapStage 7)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 7 (MapPartitionsRDD[49] at countByKey at ColumnProfiler.scala:566), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  MemoryStore:54 - Block broadcast_10 stored as values in memory (estimated size 19.2 KB, free 1458.0 MB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  MemoryStore:54 - Block broadcast_10_piece0 stored as bytes in memory (estimated size 9.5 KB, free 1458.0 MB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  BlockManagerInfo:54 - Added broadcast_10_piece0 in memory on 10.0.122.144:40967 (size: 9.5 KB, free: 1458.5 MB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  SparkContext:54 - Created broadcast 10 from broadcast at DAGScheduler.scala:1039\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[49] at countByKey at ColumnProfiler.scala:566) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  YarnScheduler:54 - Adding task set 7.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  TaskSetManager:54 - Starting task 0.0 in stage 7.0 (TID 7, algo-1, executor 1, partition 0, PROCESS_LOCAL, 8341 bytes)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:03 INFO  BlockManagerInfo:54 - Added broadcast_10_piece0 in memory on algo-1:43055 (size: 9.5 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  TaskSetManager:54 - Finished task 0.0 in stage 7.0 (TID 7) in 935 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  YarnScheduler:54 - Removed TaskSet 7.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  DAGScheduler:54 - ShuffleMapStage 7 (countByKey at ColumnProfiler.scala:566) finished in 0.955 s\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  DAGScheduler:54 - looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  DAGScheduler:54 - running: Set()\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  DAGScheduler:54 - waiting: Set(ResultStage 8)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  DAGScheduler:54 - failed: Set()\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  DAGScheduler:54 - Submitting ResultStage 8 (ShuffledRDD[50] at countByKey at ColumnProfiler.scala:566), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  MemoryStore:54 - Block broadcast_11 stored as values in memory (estimated size 3.2 KB, free 1458.0 MB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  MemoryStore:54 - Block broadcast_11_piece0 stored as bytes in memory (estimated size 1924.0 B, free 1458.0 MB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  BlockManagerInfo:54 - Added broadcast_11_piece0 in memory on 10.0.122.144:40967 (size: 1924.0 B, free: 1458.5 MB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  SparkContext:54 - Created broadcast 11 from broadcast at DAGScheduler.scala:1039\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 8 (ShuffledRDD[50] at countByKey at ColumnProfiler.scala:566) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  YarnScheduler:54 - Adding task set 8.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  TaskSetManager:54 - Starting task 0.0 in stage 8.0 (TID 8, algo-1, executor 1, partition 0, NODE_LOCAL, 7660 bytes)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  BlockManagerInfo:54 - Added broadcast_11_piece0 in memory on algo-1:43055 (size: 1924.0 B, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  MapOutputTrackerMasterEndpoint:54 - Asked to send map output locations for shuffle 2 to 10.0.122.144:57610\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  TaskSetManager:54 - Finished task 0.0 in stage 8.0 (TID 8) in 81 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  YarnScheduler:54 - Removed TaskSet 8.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  DAGScheduler:54 - ResultStage 8 (countByKey at ColumnProfiler.scala:566) finished in 0.096 s\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  DAGScheduler:54 - Job 5 finished: countByKey at ColumnProfiler.scala:566, took 1.063598 s\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  ConstraintGenerator:45 - Generating Constraints:\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  ConstraintGenerator:50 - Constraints: {\n",
      "  \"version\" : 0.0,\n",
      "  \"features\" : [ {\n",
      "    \"name\" : \"mass\",\n",
      "    \"inferred_type\" : \"Fractional\",\n",
      "    \"completeness\" : 1.0,\n",
      "    \"num_constraints\" : {\n",
      "      \"is_non_negative\" : true\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"width\",\n",
      "    \"inferred_type\" : \"Fractional\",\n",
      "    \"completeness\" : 1.0,\n",
      "    \"num_constraints\" : {\n",
      "      \"is_non_negative\" : true\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"height\",\n",
      "    \"inferred_type\" : \"Fractional\",\n",
      "    \"completeness\" : 1.0,\n",
      "    \"num_constraints\" : {\n",
      "      \"is_non_negative\" : true\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"color_score\",\n",
      "    \"inferred_type\" : \"Fractional\",\n",
      "    \"completeness\" : 1.0,\n",
      "    \"num_constraints\" : {\n",
      "      \"is_non_negative\" : true\n",
      "    }\n",
      "  } ],\n",
      "  \"monitoring_config\" : {\n",
      "    \"evaluate_constraints\" : \"Enabled\",\n",
      "    \"emit_metrics\" : \"Enabled\",\n",
      "    \"datatype_check_threshold\" : 1.0,\n",
      "    \"domain_content_threshold\" : 1.0,\n",
      "    \"distribution_constraints\" : {\n",
      "      \"perform_comparison\" : \"Enabled\",\n",
      "      \"comparison_threshold\" : 0.1,\n",
      "      \"comparison_method\" : \"Robust\"\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  FileUtil:29 - Write to file constraints.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  StatsGenerator:65 - Generating Stats:\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  CodeGenerator:54 - Code generated in 11.36465 ms\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  CodeGenerator:54 - Code generated in 9.407154 ms\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  SparkContext:54 - Starting job: count at StatsGenerator.scala:67\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  DAGScheduler:54 - Registering RDD 55 (count at StatsGenerator.scala:67)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  DAGScheduler:54 - Got job 6 (count at StatsGenerator.scala:67) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  DAGScheduler:54 - Final stage: ResultStage 10 (count at StatsGenerator.scala:67)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  DAGScheduler:54 - Missing parents: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 9 (MapPartitionsRDD[55] at count at StatsGenerator.scala:67), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  MemoryStore:54 - Block broadcast_12 stored as values in memory (estimated size 20.2 KB, free 1458.0 MB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  MemoryStore:54 - Block broadcast_12_piece0 stored as bytes in memory (estimated size 9.5 KB, free 1458.0 MB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  BlockManagerInfo:54 - Added broadcast_12_piece0 in memory on 10.0.122.144:40967 (size: 9.5 KB, free: 1458.5 MB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  SparkContext:54 - Created broadcast 12 from broadcast at DAGScheduler.scala:1039\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[55] at count at StatsGenerator.scala:67) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  YarnScheduler:54 - Adding task set 9.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  TaskSetManager:54 - Starting task 0.0 in stage 9.0 (TID 9, algo-1, executor 1, partition 0, PROCESS_LOCAL, 8341 bytes)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  BlockManagerInfo:54 - Added broadcast_12_piece0 in memory on algo-1:43055 (size: 9.5 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  TaskSetManager:54 - Finished task 0.0 in stage 9.0 (TID 9) in 61 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  YarnScheduler:54 - Removed TaskSet 9.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  DAGScheduler:54 - ShuffleMapStage 9 (count at StatsGenerator.scala:67) finished in 0.072 s\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  DAGScheduler:54 - looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  DAGScheduler:54 - running: Set()\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  DAGScheduler:54 - waiting: Set(ResultStage 10)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  DAGScheduler:54 - failed: Set()\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  DAGScheduler:54 - Submitting ResultStage 10 (MapPartitionsRDD[58] at count at StatsGenerator.scala:67), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  MemoryStore:54 - Block broadcast_13 stored as values in memory (estimated size 7.4 KB, free 1457.9 MB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  MemoryStore:54 - Block broadcast_13_piece0 stored as bytes in memory (estimated size 3.8 KB, free 1457.9 MB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  BlockManagerInfo:54 - Added broadcast_13_piece0 in memory on 10.0.122.144:40967 (size: 3.8 KB, free: 1458.5 MB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  SparkContext:54 - Created broadcast 13 from broadcast at DAGScheduler.scala:1039\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[58] at count at StatsGenerator.scala:67) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  YarnScheduler:54 - Adding task set 10.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  TaskSetManager:54 - Starting task 0.0 in stage 10.0 (TID 10, algo-1, executor 1, partition 0, NODE_LOCAL, 7765 bytes)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  BlockManagerInfo:54 - Added broadcast_13_piece0 in memory on algo-1:43055 (size: 3.8 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  MapOutputTrackerMasterEndpoint:54 - Asked to send map output locations for shuffle 3 to 10.0.122.144:57610\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  TaskSetManager:54 - Finished task 0.0 in stage 10.0 (TID 10) in 43 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  YarnScheduler:54 - Removed TaskSet 10.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  DAGScheduler:54 - ResultStage 10 (count at StatsGenerator.scala:67) finished in 0.053 s\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  DAGScheduler:54 - Job 6 finished: count at StatsGenerator.scala:67, took 0.133866 s\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  StatsGenerator:70 - Stats: {\n",
      "  \"version\" : 0.0,\n",
      "  \"dataset\" : {\n",
      "    \"item_count\" : 44\n",
      "  },\n",
      "  \"features\" : [ {\n",
      "    \"name\" : \"mass\",\n",
      "    \"inferred_type\" : \"Fractional\",\n",
      "    \"numerical_statistics\" : {\n",
      "      \"common\" : {\n",
      "        \"num_present\" : 44,\n",
      "        \"num_missing\" : 0\n",
      "      },\n",
      "      \"mean\" : 0.3019480519480519,\n",
      "      \"sum\" : 13.285714285714285,\n",
      "      \"std_dev\" : 0.18439443357765442,\n",
      "      \"min\" : 0.0,\n",
      "      \"max\" : 1.0,\n",
      "      \"distribution\" : {\n",
      "        \"kll\" : {\n",
      "          \"buckets\" : [ {\n",
      "            \"lower_bound\" : 0.0,\n",
      "            \"upper_bound\" : 0.1,\n",
      "            \"count\" : 4.0\n",
      "          }, {\n",
      "            \"lower_bound\" : 0.1,\n",
      "            \"upper_bound\" : 0.2,\n",
      "            \"count\" : 5.0\n",
      "          }, {\n",
      "            \"lower_bound\" : 0.2,\n",
      "            \"upper_bound\" : 0.3,\n",
      "            \"count\" : 15.0\n",
      "          }, {\n",
      "            \"lower_bound\" : 0.3,\n",
      "            \"upper_bound\" : 0.4,\n",
      "            \"count\" : 13.0\n",
      "          }, {\n",
      "            \"lower_bound\" : 0.4,\n",
      "            \"upper_bound\" : 0.5,\n",
      "            \"count\" : 5.0\n",
      "          }, {\n",
      "            \"lower_bound\" : 0.5,\n",
      "            \"upper_bound\" : 0.6,\n",
      "            \"count\" : 0.0\n",
      "          }, {\n",
      "            \"lower_bound\" : 0.6,\n",
      "            \"upper_bound\" : 0.7,\n",
      "            \"count\" : 0.0\n",
      "          }, {\n",
      "            \"lower_bound\" : 0.7,\n",
      "            \"upper_bound\" : 0.8,\n",
      "            \"count\" : 0.0\n",
      "          }, {\n",
      "            \"lower_bound\" : 0.8,\n",
      "            \"upper_bound\" : 0.9,\n",
      "            \"count\" : 0.0\n",
      "          }, {\n",
      "            \"lower_bound\" : 0.9,\n",
      "            \"upper_bound\" : 1.0,\n",
      "            \"count\" : 2.0\n",
      "          } ],\n",
      "          \"sketch\" : {\n",
      "            \"parameters\" : {\n",
      "              \"c\" : 0.64,\n",
      "              \"k\" : 2048.0\n",
      "            },\n",
      "            \"data\" : [ [ 0.32142857142857145, 0.42142857142857143, 0.36428571428571427, 0.2785714285714285, 0.19285714285714284, 0.2, 0.28571428571428575, 0.3714285714285714, 0.01428571428571429, 0.24285714285714283, 0.2785714285714285, 0.49999999999999994, 0.0, 0.14285714285714285, 0.3, 0.95, 0.28571428571428575, 0.14285714285714285, 0.2642857142857143, 0.3, 0.2714285714285714, 0.0357142857142857, 0.02857142857142858, 0.28571428571428575, 0.4428571428571429, 0.3, 0.2714285714285714, 0.15000000000000002, 1.0, 0.15000000000000002, 0.35000000000000003, 0.3428571428571428, 0.42857142857142855, 0.31428571428571433, 0.4071428571428572, 0.2285714285714286, 0.2785714285714285, 0.3071428571428571, 0.32857142857142857, 0.29285714285714287, 0.2357142857142857, 0.2285714285714286, 0.35714285714285715, 0.39285714285714285 ] ]\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"width\",\n",
      "    \"inferred_type\" : \"Fractional\",\n",
      "    \"numerical_statistics\" : {\n",
      "      \"common\" : {\n",
      "        \"num_present\" : 44,\n",
      "        \"num_missing\" : 0\n",
      "      },\n",
      "      \"mean\" : 0.3709893048128343,\n",
      "      \"sum\" : 16.32352941176471,\n",
      "      \"std_dev\" : 0.21615108704431354,\n",
      "      \"min\" : 0.0,\n",
      "      \"max\" : 1.0000000000000002,\n",
      "      \"distribution\" : {\n",
      "        \"kll\" : {\n",
      "          \"buckets\" : [ {\n",
      "            \"lower_bound\" : 0.0,\n",
      "            \"upper_bound\" : 0.10000000000000002,\n",
      "            \"count\" : 8.0\n",
      "          }, {\n",
      "            \"lower_bound\" : 0.10000000000000002,\n",
      "            \"upper_bound\" : 0.20000000000000004,\n",
      "            \"count\" : 2.0\n",
      "          }, {\n",
      "            \"lower_bound\" : 0.20000000000000004,\n",
      "            \"upper_bound\" : 0.3000000000000001,\n",
      "            \"count\" : 3.0\n",
      "          }, {\n",
      "            \"lower_bound\" : 0.3000000000000001,\n",
      "            \"upper_bound\" : 0.4000000000000001,\n",
      "            \"count\" : 7.0\n",
      "          }, {\n",
      "            \"lower_bound\" : 0.4000000000000001,\n",
      "            \"upper_bound\" : 0.5000000000000001,\n",
      "            \"count\" : 14.0\n",
      "          }, {\n",
      "            \"lower_bound\" : 0.5000000000000001,\n",
      "            \"upper_bound\" : 0.6000000000000002,\n",
      "            \"count\" : 7.0\n",
      "          }, {\n",
      "            \"lower_bound\" : 0.6000000000000002,\n",
      "            \"upper_bound\" : 0.7000000000000002,\n",
      "            \"count\" : 1.0\n",
      "          }, {\n",
      "            \"lower_bound\" : 0.7000000000000002,\n",
      "            \"upper_bound\" : 0.8000000000000002,\n",
      "            \"count\" : 0.0\n",
      "          }, {\n",
      "            \"lower_bound\" : 0.8000000000000002,\n",
      "            \"upper_bound\" : 0.9000000000000001,\n",
      "            \"count\" : 0.0\n",
      "          }, {\n",
      "            \"lower_bound\" : 0.9000000000000001,\n",
      "            \"upper_bound\" : 1.0000000000000002,\n",
      "            \"count\" : 2.0\n",
      "          } ],\n",
      "          \"sketch\" : {\n",
      "            \"parameters\" : {\n",
      "              \"c\" : 0.64,\n",
      "              \"k\" : 2048.0\n",
      "            },\n",
      "            \"data\" : [ [ 0.32352941176470584, 0.41176470588235303, 0.3823529411764708, 0.35294117647058854, 0.05882352941176472, 0.0, 0.470588235294118, 0.6470588235294119, 0.02941176470588247, 0.2941176470588236, 0.3823529411764708, 0.4411764705882353, 0.0, 0.08823529411764697, 0.35294117647058854, 0.9411764705882353, 0.5588235294117647, 0.1470588235294117, 0.3823529411764708, 0.5000000000000002, 0.2058823529411764, 0.11764705882352944, 0.05882352941176472, 0.5294117647058825, 0.4411764705882353, 0.3823529411764708, 0.5294117647058825, 0.02941176470588247, 1.0000000000000002, 0.08823529411764697, 0.4411764705882353, 0.470588235294118, 0.4411764705882353, 0.41176470588235303, 0.5000000000000002, 0.4411764705882353, 0.41176470588235303, 0.470588235294118, 0.5000000000000002, 0.41176470588235303, 0.5294117647058825, 0.26470588235294135, 0.470588235294118, 0.41176470588235303 ] ]\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"height\",\n",
      "    \"inferred_type\" : \"Fractional\",\n",
      "    \"numerical_statistics\" : {\n",
      "      \"common\" : {\n",
      "        \"num_present\" : 44,\n",
      "        \"num_missing\" : 0\n",
      "      },\n",
      "      \"mean\" : 0.5685314685314685,\n",
      "      \"sum\" : 25.015384615384615,\n",
      "      \"std_dev\" : 0.21951471111635912,\n",
      "      \"min\" : 0.0,\n",
      "      \"max\" : 1.0,\n",
      "      \"distribution\" : {\n",
      "        \"kll\" : {\n",
      "          \"buckets\" : [ {\n",
      "            \"lower_bound\" : 0.0,\n",
      "            \"upper_bound\" : 0.1,\n",
      "            \"count\" : 3.0\n",
      "          }, {\n",
      "            \"lower_bound\" : 0.1,\n",
      "            \"upper_bound\" : 0.2,\n",
      "            \"count\" : 1.0\n",
      "          }, {\n",
      "            \"lower_bound\" : 0.2,\n",
      "            \"upper_bound\" : 0.3,\n",
      "            \"count\" : 0.0\n",
      "          }, {\n",
      "            \"lower_bound\" : 0.3,\n",
      "            \"upper_bound\" : 0.4,\n",
      "            \"count\" : 0.0\n",
      "          }, {\n",
      "            \"lower_bound\" : 0.4,\n",
      "            \"upper_bound\" : 0.5,\n",
      "            \"count\" : 10.0\n",
      "          }, {\n",
      "            \"lower_bound\" : 0.5,\n",
      "            \"upper_bound\" : 0.6,\n",
      "            \"count\" : 14.0\n",
      "          }, {\n",
      "            \"lower_bound\" : 0.6,\n",
      "            \"upper_bound\" : 0.7,\n",
      "            \"count\" : 7.0\n",
      "          }, {\n",
      "            \"lower_bound\" : 0.7,\n",
      "            \"upper_bound\" : 0.8,\n",
      "            \"count\" : 1.0\n",
      "          }, {\n",
      "            \"lower_bound\" : 0.8,\n",
      "            \"upper_bound\" : 0.9,\n",
      "            \"count\" : 4.0\n",
      "          }, {\n",
      "            \"lower_bound\" : 0.9,\n",
      "            \"upper_bound\" : 1.0,\n",
      "            \"count\" : 4.0\n",
      "          } ],\n",
      "          \"sketch\" : {\n",
      "            \"parameters\" : {\n",
      "              \"c\" : 0.64,\n",
      "              \"k\" : 2048.0\n",
      "            },\n",
      "            \"data\" : [ [ 0.5076923076923077, 0.9692307692307693, 0.5846153846153845, 0.476923076923077, 0.6461538461538461, 0.723076923076923, 0.5230769230769232, 0.4307692307692308, 0.0461538461538461, 0.5230769230769232, 0.5384615384615385, 0.9538461538461538, 0.0, 0.6923076923076923, 0.5230769230769232, 0.8307692307692309, 0.476923076923077, 0.5692307692307692, 0.6000000000000001, 0.5384615384615385, 0.6923076923076923, 0.10769230769230775, 0.09230769230769231, 0.5384615384615385, 1.0, 0.5538461538461539, 0.5076923076923077, 0.6153846153846154, 0.8, 0.6307692307692307, 0.9384615384615385, 0.4615384615384617, 0.8769230769230769, 0.4615384615384617, 0.6307692307692307, 0.476923076923077, 0.49230769230769234, 0.49230769230769234, 0.5538461538461539, 0.5846153846153845, 0.5846153846153845, 0.476923076923077, 0.49230769230769234, 0.8 ] ]\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"color_score\",\n",
      "    \"inferred_type\" : \"Fractional\",\n",
      "    \"numerical_statistics\" : {\n",
      "      \"common\" : {\n",
      "        \"num_present\" : 44,\n",
      "        \"num_missing\" : 0\n",
      "      },\n",
      "      \"mean\" : 0.5020053475935828,\n",
      "      \"sum\" : 22.088235294117645,\n",
      "      \"std_dev\" : 0.21752758384968102,\n",
      "      \"min\" : 0.0,\n",
      "      \"max\" : 1.0,\n",
      "      \"distribution\" : {\n",
      "        \"kll\" : {\n",
      "          \"buckets\" : [ {\n",
      "            \"lower_bound\" : 0.0,\n",
      "            \"upper_bound\" : 0.1,\n",
      "            \"count\" : 2.0\n",
      "          }, {\n",
      "            \"lower_bound\" : 0.1,\n",
      "            \"upper_bound\" : 0.2,\n",
      "            \"count\" : 0.0\n",
      "          }, {\n",
      "            \"lower_bound\" : 0.2,\n",
      "            \"upper_bound\" : 0.3,\n",
      "            \"count\" : 3.0\n",
      "          }, {\n",
      "            \"lower_bound\" : 0.3,\n",
      "            \"upper_bound\" : 0.4,\n",
      "            \"count\" : 13.0\n",
      "          }, {\n",
      "            \"lower_bound\" : 0.4,\n",
      "            \"upper_bound\" : 0.5,\n",
      "            \"count\" : 8.0\n",
      "          }, {\n",
      "            \"lower_bound\" : 0.5,\n",
      "            \"upper_bound\" : 0.6,\n",
      "            \"count\" : 4.0\n",
      "          }, {\n",
      "            \"lower_bound\" : 0.6,\n",
      "            \"upper_bound\" : 0.7,\n",
      "            \"count\" : 6.0\n",
      "          }, {\n",
      "            \"lower_bound\" : 0.7,\n",
      "            \"upper_bound\" : 0.8,\n",
      "            \"count\" : 3.0\n",
      "          }, {\n",
      "            \"lower_bound\" : 0.8,\n",
      "            \"upper_bound\" : 0.9,\n",
      "            \"count\" : 3.0\n",
      "          }, {\n",
      "            \"lower_bound\" : 0.9,\n",
      "            \"upper_bound\" : 1.0,\n",
      "            \"count\" : 2.0\n",
      "          } ],\n",
      "          \"sketch\" : {\n",
      "            \"parameters\" : {\n",
      "              \"c\" : 0.64,\n",
      "              \"k\" : 2048.0\n",
      "            },\n",
      "            \"data\" : [ [ 1.0, 0.3235294117647056, 0.9705882352941178, 0.8529411764705879, 0.35294117647058787, 0.4117647058823528, 0.7352941176470584, 0.0, 0.6470588235294117, 0.4705882352941173, 0.5588235294117645, 0.35294117647058787, 0.6470588235294117, 0.35294117647058787, 0.6470588235294117, 0.4705882352941173, 0.2941176470588234, 0.38235294117647056, 0.4705882352941173, 0.7941176470588234, 0.38235294117647056, 0.6176470588235294, 0.5882352941176472, 0.23529411764705888, 0.38235294117647056, 0.5, 0.2941176470588234, 0.38235294117647056, 0.4705882352941173, 0.3235294117647056, 0.38235294117647056, 0.8823529411764706, 0.38235294117647056, 0.6176470588235294, 0.44117647058823506, 0.8235294117647056, 0.6764705882352939, 0.7647058823529411, 0.4117647058823528, 0.5294117647058822, 0.4705882352941173, 0.38235294117647056, 0.02941176470588225, 0.38235294117647056 ] ]\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  } ]\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  FileUtil:29 - Write to file statistics.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:04 INFO  YarnClientSchedulerBackend:54 - Interrupting monitor thread\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:05 INFO  YarnClientSchedulerBackend:54 - Shutting down all executors\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:05 INFO  YarnSchedulerBackend$YarnDriverEndpoint:54 - Asking each executor to shut down\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:05 INFO  SchedulerExtensionServices:54 - Stopping SchedulerExtensionServices\u001b[0m\n",
      "\u001b[34m(serviceOption=None,\n",
      " services=List(),\n",
      " started=false)\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:05 INFO  YarnClientSchedulerBackend:54 - Stopped\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:05 INFO  MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:05 INFO  MemoryStore:54 - MemoryStore cleared\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:05 INFO  BlockManager:54 - BlockManager stopped\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:05 INFO  BlockManagerMaster:54 - BlockManagerMaster stopped\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:05 INFO  OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:05 INFO  SparkContext:54 - Successfully stopped SparkContext\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:05 INFO  Main:62 - Completed: Job completed successfully with no violations.\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:05 INFO  Main:138 - Write to file /opt/ml/output/message.\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:05 INFO  ShutdownHookManager:54 - Shutdown hook called\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:05 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-c4f1312e-0a74-4970-9a00-8a3c497ddbf9\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:05 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-cd213f05-d020-4061-93c0-a0e02639e254\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:05,403 - DefaultDataAnalyzer - INFO - Completed spark-submit with return code : 0\u001b[0m\n",
      "\u001b[34m2020-11-08 02:24:05,404 - DefaultDataAnalyzer - INFO - Spark job completed.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sagemaker.processing.ProcessingJob at 0x7fbd3cb52550>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_model_monitor.suggest_baseline(\n",
    "                            baseline_dataset=baseline_data_uri + '/train_with_header.csv',\n",
    "                            dataset_format=DatasetFormat.csv(header=True),\n",
    "                            output_s3_uri=baseline_results_uri,\n",
    "                            wait=True\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the generated constraints and statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.Session().client('s3')\n",
    "result = s3_client.list_objects(Bucket=bucket, Prefix=baseline_results_prefix)\n",
    "report_files = [report_file.get('Key') for report_file in result.get('Contents')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Files:\n",
      "classifier/model-monitor/baselining/results/constraints.json\n",
      " classifier/model-monitor/baselining/results/statistics.json\n"
     ]
    }
   ],
   "source": [
    "print('Found Files:')\n",
    "print(\"\\n \".join(report_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_job = default_model_monitor.latest_baselining_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inputs': [<sagemaker.processing.ProcessingInput at 0x7fbd3cb52588>],\n",
       " 'outputs': [<sagemaker.processing.ProcessingOutput at 0x7fbd3cb525f8>],\n",
       " 'output_kms_key': None,\n",
       " 'sagemaker_session': <sagemaker.session.Session at 0x7fbd3cb945f8>,\n",
       " 'job_name': 'baseline-suggestion-job-2020-11-08-02-19-12-931'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_job.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'session' will be renamed to 'sagemaker_session' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'evaluate_constraints': 'Enabled',\n",
       " 'emit_metrics': 'Enabled',\n",
       " 'datatype_check_threshold': 1.0,\n",
       " 'domain_content_threshold': 1.0,\n",
       " 'distribution_constraints': {'perform_comparison': 'Enabled',\n",
       "  'comparison_threshold': 0.1,\n",
       "  'comparison_method': 'Robust'}}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_job.suggested_constraints().body_dict['monitoring_config']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'session' will be renamed to 'sagemaker_session' in SageMaker Python SDK v2.\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:1: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>inferred_type</th>\n",
       "      <th>numerical_statistics.common.num_present</th>\n",
       "      <th>numerical_statistics.common.num_missing</th>\n",
       "      <th>numerical_statistics.mean</th>\n",
       "      <th>numerical_statistics.sum</th>\n",
       "      <th>numerical_statistics.std_dev</th>\n",
       "      <th>numerical_statistics.min</th>\n",
       "      <th>numerical_statistics.max</th>\n",
       "      <th>numerical_statistics.distribution.kll.buckets</th>\n",
       "      <th>numerical_statistics.distribution.kll.sketch.parameters.c</th>\n",
       "      <th>numerical_statistics.distribution.kll.sketch.parameters.k</th>\n",
       "      <th>numerical_statistics.distribution.kll.sketch.data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mass</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>0.301948</td>\n",
       "      <td>13.285714</td>\n",
       "      <td>0.184394</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 0.1, 'count': 4.0}, {'lower_bound': 0.1, 'upper_bound': 0.2, 'count': 5.0}, {'lower_bound': 0.2, 'upper_bound': 0.3, 'count': 15.0}, {'lower_bound': 0.3, 'upper_bound': 0.4, 'count': 13.0}, {'lower_bound': 0.4, 'upper_bound': 0.5, 'count': 5.0}, {'lower_bound': 0.5, 'upper_bound': 0.6, 'count': 0.0}, {'lower_bound': 0.6, 'upper_bound': 0.7, 'count': 0.0}, {'lower_bound': 0.7, 'upper_bound': 0.8, 'count': 0.0}, {'lower_bound': 0.8, 'upper_bound': 0.9, 'count': 0.0}, {'lower_bound': 0.9, 'upper_bound': 1.0, 'count': 2.0}]</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[0.32142857142857145, 0.42142857142857143, 0.36428571428571427, 0.2785714285714285, 0.19285714285714284, 0.2, 0.28571428571428575, 0.3714285714285714, 0.01428571428571429, 0.24285714285714283, 0.2785714285714285, 0.49999999999999994, 0.0, 0.14285714285714285, 0.3, 0.95, 0.28571428571428575, 0.14285714285714285, 0.2642857142857143, 0.3, 0.2714285714285714, 0.0357142857142857, 0.02857142857142858, 0.28571428571428575, 0.4428571428571429, 0.3, 0.2714285714285714, 0.15000000000000002, 1.0, 0.15000000000000002, 0.35000000000000003, 0.3428571428571428, 0.42857142857142855, 0.31428571428571433, 0.4071428571428572, 0.2285714285714286, 0.2785714285714285, 0.3071428571428571, 0.32857142857142857, 0.29285714285714287, 0.2357142857142857, 0.2285714285714286, 0.35714285714285715, 0.39285714285714285]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>width</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>0.370989</td>\n",
       "      <td>16.323529</td>\n",
       "      <td>0.216151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 0.10000000000000002, 'count': 8.0}, {'lower_bound': 0.10000000000000002, 'upper_bound': 0.20000000000000004, 'count': 2.0}, {'lower_bound': 0.20000000000000004, 'upper_bound': 0.3000000000000001, 'count': 3.0}, {'lower_bound': 0.3000000000000001, 'upper_bound': 0.4000000000000001, 'count': 7.0}, {'lower_bound': 0.4000000000000001, 'upper_bound': 0.5000000000000001, 'count': 14.0}, {'lower_bound': 0.5000000000000001, 'upper_bound': 0.6000000000000002, 'count': 7.0}, {'lower_bound': 0.6000000000000002, 'upper_bound': 0.7000000000000002, 'count': 1.0}, {'lower_bound': 0.7000000000000002, 'upper_bound': 0.8000000000000002, 'count': 0.0}, {'lower_bound': 0.8000000000000002, 'upper_bound': 0.9000000000000001, 'count': 0.0}, {'lower_bound': 0.9000000000000001, 'upper_bound': 1.0000000000000002, 'count': 2.0}]</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[0.32352941176470584, 0.41176470588235303, 0.3823529411764708, 0.35294117647058854, 0.05882352941176472, 0.0, 0.470588235294118, 0.6470588235294119, 0.02941176470588247, 0.2941176470588236, 0.3823529411764708, 0.4411764705882353, 0.0, 0.08823529411764697, 0.35294117647058854, 0.9411764705882353, 0.5588235294117647, 0.1470588235294117, 0.3823529411764708, 0.5000000000000002, 0.2058823529411764, 0.11764705882352944, 0.05882352941176472, 0.5294117647058825, 0.4411764705882353, 0.3823529411764708, 0.5294117647058825, 0.02941176470588247, 1.0000000000000002, 0.08823529411764697, 0.4411764705882353, 0.470588235294118, 0.4411764705882353, 0.41176470588235303, 0.5000000000000002, 0.4411764705882353, 0.41176470588235303, 0.470588235294118, 0.5000000000000002, 0.41176470588235303, 0.5294117647058825, 0.26470588235294135, 0.470588235294118, 0.41176470588235303]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>height</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>0.568531</td>\n",
       "      <td>25.015385</td>\n",
       "      <td>0.219515</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 0.1, 'count': 3.0}, {'lower_bound': 0.1, 'upper_bound': 0.2, 'count': 1.0}, {'lower_bound': 0.2, 'upper_bound': 0.3, 'count': 0.0}, {'lower_bound': 0.3, 'upper_bound': 0.4, 'count': 0.0}, {'lower_bound': 0.4, 'upper_bound': 0.5, 'count': 10.0}, {'lower_bound': 0.5, 'upper_bound': 0.6, 'count': 14.0}, {'lower_bound': 0.6, 'upper_bound': 0.7, 'count': 7.0}, {'lower_bound': 0.7, 'upper_bound': 0.8, 'count': 1.0}, {'lower_bound': 0.8, 'upper_bound': 0.9, 'count': 4.0}, {'lower_bound': 0.9, 'upper_bound': 1.0, 'count': 4.0}]</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[0.5076923076923077, 0.9692307692307693, 0.5846153846153845, 0.476923076923077, 0.6461538461538461, 0.723076923076923, 0.5230769230769232, 0.4307692307692308, 0.0461538461538461, 0.5230769230769232, 0.5384615384615385, 0.9538461538461538, 0.0, 0.6923076923076923, 0.5230769230769232, 0.8307692307692309, 0.476923076923077, 0.5692307692307692, 0.6000000000000001, 0.5384615384615385, 0.6923076923076923, 0.10769230769230775, 0.09230769230769231, 0.5384615384615385, 1.0, 0.5538461538461539, 0.5076923076923077, 0.6153846153846154, 0.8, 0.6307692307692307, 0.9384615384615385, 0.4615384615384617, 0.8769230769230769, 0.4615384615384617, 0.6307692307692307, 0.476923076923077, 0.49230769230769234, 0.49230769230769234, 0.5538461538461539, 0.5846153846153845, 0.5846153846153845, 0.476923076923077, 0.49230769230769234, 0.8]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>color_score</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>0.502005</td>\n",
       "      <td>22.088235</td>\n",
       "      <td>0.217528</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 0.1, 'count': 2.0}, {'lower_bound': 0.1, 'upper_bound': 0.2, 'count': 0.0}, {'lower_bound': 0.2, 'upper_bound': 0.3, 'count': 3.0}, {'lower_bound': 0.3, 'upper_bound': 0.4, 'count': 13.0}, {'lower_bound': 0.4, 'upper_bound': 0.5, 'count': 8.0}, {'lower_bound': 0.5, 'upper_bound': 0.6, 'count': 4.0}, {'lower_bound': 0.6, 'upper_bound': 0.7, 'count': 6.0}, {'lower_bound': 0.7, 'upper_bound': 0.8, 'count': 3.0}, {'lower_bound': 0.8, 'upper_bound': 0.9, 'count': 3.0}, {'lower_bound': 0.9, 'upper_bound': 1.0, 'count': 2.0}]</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[1.0, 0.3235294117647056, 0.9705882352941178, 0.8529411764705879, 0.35294117647058787, 0.4117647058823528, 0.7352941176470584, 0.0, 0.6470588235294117, 0.4705882352941173, 0.5588235294117645, 0.35294117647058787, 0.6470588235294117, 0.35294117647058787, 0.6470588235294117, 0.4705882352941173, 0.2941176470588234, 0.38235294117647056, 0.4705882352941173, 0.7941176470588234, 0.38235294117647056, 0.6176470588235294, 0.5882352941176472, 0.23529411764705888, 0.38235294117647056, 0.5, 0.2941176470588234, 0.38235294117647056, 0.4705882352941173, 0.3235294117647056, 0.38235294117647056, 0.8823529411764706, 0.38235294117647056, 0.6176470588235294, 0.44117647058823506, 0.8235294117647056, 0.6764705882352939, 0.7647058823529411, 0.4117647058823528, 0.5294117647058822, 0.4705882352941173, 0.38235294117647056, 0.02941176470588225, 0.38235294117647056]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          name inferred_type  numerical_statistics.common.num_present  \\\n",
       "0  mass         Fractional    44                                        \n",
       "1  width        Fractional    44                                        \n",
       "2  height       Fractional    44                                        \n",
       "3  color_score  Fractional    44                                        \n",
       "\n",
       "   numerical_statistics.common.num_missing  numerical_statistics.mean  \\\n",
       "0  0                                        0.301948                    \n",
       "1  0                                        0.370989                    \n",
       "2  0                                        0.568531                    \n",
       "3  0                                        0.502005                    \n",
       "\n",
       "   numerical_statistics.sum  numerical_statistics.std_dev  \\\n",
       "0  13.285714                 0.184394                       \n",
       "1  16.323529                 0.216151                       \n",
       "2  25.015385                 0.219515                       \n",
       "3  22.088235                 0.217528                       \n",
       "\n",
       "   numerical_statistics.min  numerical_statistics.max  \\\n",
       "0  0.0                       1.0                        \n",
       "1  0.0                       1.0                        \n",
       "2  0.0                       1.0                        \n",
       "3  0.0                       1.0                        \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        numerical_statistics.distribution.kll.buckets  \\\n",
       "0  [{'lower_bound': 0.0, 'upper_bound': 0.1, 'count': 4.0}, {'lower_bound': 0.1, 'upper_bound': 0.2, 'count': 5.0}, {'lower_bound': 0.2, 'upper_bound': 0.3, 'count': 15.0}, {'lower_bound': 0.3, 'upper_bound': 0.4, 'count': 13.0}, {'lower_bound': 0.4, 'upper_bound': 0.5, 'count': 5.0}, {'lower_bound': 0.5, 'upper_bound': 0.6, 'count': 0.0}, {'lower_bound': 0.6, 'upper_bound': 0.7, 'count': 0.0}, {'lower_bound': 0.7, 'upper_bound': 0.8, 'count': 0.0}, {'lower_bound': 0.8, 'upper_bound': 0.9, 'count': 0.0}, {'lower_bound': 0.9, 'upper_bound': 1.0, 'count': 2.0}]                                                                                                                                                                                                                                                                                                   \n",
       "1  [{'lower_bound': 0.0, 'upper_bound': 0.10000000000000002, 'count': 8.0}, {'lower_bound': 0.10000000000000002, 'upper_bound': 0.20000000000000004, 'count': 2.0}, {'lower_bound': 0.20000000000000004, 'upper_bound': 0.3000000000000001, 'count': 3.0}, {'lower_bound': 0.3000000000000001, 'upper_bound': 0.4000000000000001, 'count': 7.0}, {'lower_bound': 0.4000000000000001, 'upper_bound': 0.5000000000000001, 'count': 14.0}, {'lower_bound': 0.5000000000000001, 'upper_bound': 0.6000000000000002, 'count': 7.0}, {'lower_bound': 0.6000000000000002, 'upper_bound': 0.7000000000000002, 'count': 1.0}, {'lower_bound': 0.7000000000000002, 'upper_bound': 0.8000000000000002, 'count': 0.0}, {'lower_bound': 0.8000000000000002, 'upper_bound': 0.9000000000000001, 'count': 0.0}, {'lower_bound': 0.9000000000000001, 'upper_bound': 1.0000000000000002, 'count': 2.0}]   \n",
       "2  [{'lower_bound': 0.0, 'upper_bound': 0.1, 'count': 3.0}, {'lower_bound': 0.1, 'upper_bound': 0.2, 'count': 1.0}, {'lower_bound': 0.2, 'upper_bound': 0.3, 'count': 0.0}, {'lower_bound': 0.3, 'upper_bound': 0.4, 'count': 0.0}, {'lower_bound': 0.4, 'upper_bound': 0.5, 'count': 10.0}, {'lower_bound': 0.5, 'upper_bound': 0.6, 'count': 14.0}, {'lower_bound': 0.6, 'upper_bound': 0.7, 'count': 7.0}, {'lower_bound': 0.7, 'upper_bound': 0.8, 'count': 1.0}, {'lower_bound': 0.8, 'upper_bound': 0.9, 'count': 4.0}, {'lower_bound': 0.9, 'upper_bound': 1.0, 'count': 4.0}]                                                                                                                                                                                                                                                                                                   \n",
       "3  [{'lower_bound': 0.0, 'upper_bound': 0.1, 'count': 2.0}, {'lower_bound': 0.1, 'upper_bound': 0.2, 'count': 0.0}, {'lower_bound': 0.2, 'upper_bound': 0.3, 'count': 3.0}, {'lower_bound': 0.3, 'upper_bound': 0.4, 'count': 13.0}, {'lower_bound': 0.4, 'upper_bound': 0.5, 'count': 8.0}, {'lower_bound': 0.5, 'upper_bound': 0.6, 'count': 4.0}, {'lower_bound': 0.6, 'upper_bound': 0.7, 'count': 6.0}, {'lower_bound': 0.7, 'upper_bound': 0.8, 'count': 3.0}, {'lower_bound': 0.8, 'upper_bound': 0.9, 'count': 3.0}, {'lower_bound': 0.9, 'upper_bound': 1.0, 'count': 2.0}]                                                                                                                                                                                                                                                                                                    \n",
       "\n",
       "   numerical_statistics.distribution.kll.sketch.parameters.c  \\\n",
       "0  0.64                                                        \n",
       "1  0.64                                                        \n",
       "2  0.64                                                        \n",
       "3  0.64                                                        \n",
       "\n",
       "   numerical_statistics.distribution.kll.sketch.parameters.k  \\\n",
       "0  2048.0                                                      \n",
       "1  2048.0                                                      \n",
       "2  2048.0                                                      \n",
       "3  2048.0                                                      \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  numerical_statistics.distribution.kll.sketch.data  \n",
       "0  [[0.32142857142857145, 0.42142857142857143, 0.36428571428571427, 0.2785714285714285, 0.19285714285714284, 0.2, 0.28571428571428575, 0.3714285714285714, 0.01428571428571429, 0.24285714285714283, 0.2785714285714285, 0.49999999999999994, 0.0, 0.14285714285714285, 0.3, 0.95, 0.28571428571428575, 0.14285714285714285, 0.2642857142857143, 0.3, 0.2714285714285714, 0.0357142857142857, 0.02857142857142858, 0.28571428571428575, 0.4428571428571429, 0.3, 0.2714285714285714, 0.15000000000000002, 1.0, 0.15000000000000002, 0.35000000000000003, 0.3428571428571428, 0.42857142857142855, 0.31428571428571433, 0.4071428571428572, 0.2285714285714286, 0.2785714285714285, 0.3071428571428571, 0.32857142857142857, 0.29285714285714287, 0.2357142857142857, 0.2285714285714286, 0.35714285714285715, 0.39285714285714285]]                                                                  \n",
       "1  [[0.32352941176470584, 0.41176470588235303, 0.3823529411764708, 0.35294117647058854, 0.05882352941176472, 0.0, 0.470588235294118, 0.6470588235294119, 0.02941176470588247, 0.2941176470588236, 0.3823529411764708, 0.4411764705882353, 0.0, 0.08823529411764697, 0.35294117647058854, 0.9411764705882353, 0.5588235294117647, 0.1470588235294117, 0.3823529411764708, 0.5000000000000002, 0.2058823529411764, 0.11764705882352944, 0.05882352941176472, 0.5294117647058825, 0.4411764705882353, 0.3823529411764708, 0.5294117647058825, 0.02941176470588247, 1.0000000000000002, 0.08823529411764697, 0.4411764705882353, 0.470588235294118, 0.4411764705882353, 0.41176470588235303, 0.5000000000000002, 0.4411764705882353, 0.41176470588235303, 0.470588235294118, 0.5000000000000002, 0.41176470588235303, 0.5294117647058825, 0.26470588235294135, 0.470588235294118, 0.41176470588235303]]  \n",
       "2  [[0.5076923076923077, 0.9692307692307693, 0.5846153846153845, 0.476923076923077, 0.6461538461538461, 0.723076923076923, 0.5230769230769232, 0.4307692307692308, 0.0461538461538461, 0.5230769230769232, 0.5384615384615385, 0.9538461538461538, 0.0, 0.6923076923076923, 0.5230769230769232, 0.8307692307692309, 0.476923076923077, 0.5692307692307692, 0.6000000000000001, 0.5384615384615385, 0.6923076923076923, 0.10769230769230775, 0.09230769230769231, 0.5384615384615385, 1.0, 0.5538461538461539, 0.5076923076923077, 0.6153846153846154, 0.8, 0.6307692307692307, 0.9384615384615385, 0.4615384615384617, 0.8769230769230769, 0.4615384615384617, 0.6307692307692307, 0.476923076923077, 0.49230769230769234, 0.49230769230769234, 0.5538461538461539, 0.5846153846153845, 0.5846153846153845, 0.476923076923077, 0.49230769230769234, 0.8]]                                            \n",
       "3  [[1.0, 0.3235294117647056, 0.9705882352941178, 0.8529411764705879, 0.35294117647058787, 0.4117647058823528, 0.7352941176470584, 0.0, 0.6470588235294117, 0.4705882352941173, 0.5588235294117645, 0.35294117647058787, 0.6470588235294117, 0.35294117647058787, 0.6470588235294117, 0.4705882352941173, 0.2941176470588234, 0.38235294117647056, 0.4705882352941173, 0.7941176470588234, 0.38235294117647056, 0.6176470588235294, 0.5882352941176472, 0.23529411764705888, 0.38235294117647056, 0.5, 0.2941176470588234, 0.38235294117647056, 0.4705882352941173, 0.3235294117647056, 0.38235294117647056, 0.8823529411764706, 0.38235294117647056, 0.6176470588235294, 0.44117647058823506, 0.8235294117647056, 0.6764705882352939, 0.7647058823529411, 0.4117647058823528, 0.5294117647058822, 0.4705882352941173, 0.38235294117647056, 0.02941176470588225, 0.38235294117647056]]               "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema_df = pd.io.json.json_normalize(baseline_job.baseline_statistics().body_dict['features'])\n",
    "schema_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'session' will be renamed to 'sagemaker_session' in SageMaker Python SDK v2.\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:1: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>inferred_type</th>\n",
       "      <th>completeness</th>\n",
       "      <th>num_constraints.is_non_negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mass</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>width</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>height</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>color_score</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          name inferred_type  completeness  num_constraints.is_non_negative\n",
       "0  mass         Fractional    1.0           True                           \n",
       "1  width        Fractional    1.0           True                           \n",
       "2  height       Fractional    1.0           True                           \n",
       "3  color_score  Fractional    1.0           True                           "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constraints_df = pd.io.json.json_normalize(baseline_job.suggested_constraints().body_dict['features'])\n",
    "constraints_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring Schedules\n",
    "\n",
    "<p><b>Analyzing collected data for data quality issues</b></p>\n",
    "\n",
    "When you have collected the data above, analyze and monitor the data with Monitoring Schedules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import CronExpressionGenerator\n",
    "from time import gmtime, strftime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'session' will be renamed to 'sagemaker_session' in SageMaker Python SDK v2.\n",
      "Parameter 'session' will be renamed to 'sagemaker_session' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating Monitoring Schedule with name: clf-xgb-model-monitor-schedule-2020-11-08-02-26-29\n"
     ]
    }
   ],
   "source": [
    "mon_schedule_name = 'clf-xgb-model-monitor-schedule-' + strftime('%Y-%m-%d-%H-%M-%S', gmtime())\n",
    "\n",
    "default_model_monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name=mon_schedule_name,\n",
    "    endpoint_input=predictor.endpoint,\n",
    "    output_s3_uri=s3_report_path,\n",
    "    statistics=default_model_monitor.baseline_statistics(),\n",
    "    constraints=default_model_monitor.suggested_constraints(),\n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    "    enable_cloudwatch_metrics=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model Monitoring using Artificial Traffic\n",
    "The cell below starts a thread to send some traffic to the endpoint. Note that you need to stop the kernel to terminate this thread. If there is no traffic, the monitoring jobs are marked as `Failed` since there is no data to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "from time import sleep\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# endpoint_name = predictor.endpoint\n",
    "endpoint_name = 'classifier-xgboost-model-monitor-2020-11-08-01-54-47'\n",
    "runtime_client = boto3.client('runtime.sagemaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_endpoint(endpoint_name, file_name, runtime_client):\n",
    "    with open(file_name, 'r') as f:\n",
    "        for row in f:\n",
    "            payload = row.rstrip('\\n')\n",
    "            runtime_client.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                           ContentType='text/csv', \n",
    "                                           Body=payload)\n",
    "            #prediction = response['Body'].read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_endpoint_forever():\n",
    "    while True:\n",
    "        invoke_endpoint(endpoint_name, '.././DATA/test/model_monitor_test.csv', runtime_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = Thread(target = invoke_endpoint_forever)\n",
    "thread.start()\n",
    "# NOTE: You need to stop the kernel to stop the invocations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe and Inspect the Schedule\n",
    "Once you describe, observe that the MonitoringScheduleStatus changes to Scheduled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schedule status: Scheduled\n"
     ]
    }
   ],
   "source": [
    "desc_schedule_result = default_model_monitor.describe_schedule()\n",
    "print('Schedule status: {}'.format(desc_schedule_result['MonitoringScheduleStatus']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List executions\n",
    "The schedule starts jobs at the previously specified intervals. Here, you list the latest five executions. Note that if you are kicking this off after creating the hourly schedule, you might find the executions empty. You might have to wait until you cross the hour boundary (in UTC) to see executions kick off. The code below has the logic for waiting.\n",
    "\n",
    "Note: Even for an hourly schedule, Amazon SageMaker has a buffer period of 20 minutes to schedule your execution. You might see your execution start in anywhere from zero to ~20 minutes from the hour boundary. This is expected and done for load balancing in the backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We created a hourly schedule above and it will kick off executions ON the hour (plus 0 - 20 min buffer).\n",
      "We will have to wait till we hit the hour ...\n"
     ]
    }
   ],
   "source": [
    "mon_executions = default_model_monitor.list_executions()\n",
    "\n",
    "print(\"We created a hourly schedule above and it will kick off executions ON the hour (plus 0 - 20 min buffer).\")\n",
    "print(\"We will have to wait till we hit the hour ...\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<sagemaker.model_monitor.model_monitoring.MonitoringExecution at 0x7fbd37f95da0>]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wait till you see an execution object in this list before you proceed to the next step\n",
    "# takes between (60 to 80 mins)\n",
    "mon_executions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect a specific execution (latest execution)\n",
    "In the previous cell, you picked up the latest completed or failed scheduled execution. Here are the possible terminal states and what each of them mean: \n",
    "* Completed - This means the monitoring execution completed and no issues were found in the violations report.\n",
    "* CompletedWithViolations - This means the execution completed, but constraint violations were detected.\n",
    "* Failed - The monitoring execution failed, maybe due to client error (perhaps incorrect role premissions) or infrastructure issues. Further examination of FailureReason and ExitMessage is necessary to identify what exactly happened.\n",
    "* Stopped - Job exceeded max runtime or was manually stopped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ProcessingInputs': [{'InputName': 'input_1',\n",
       "   'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-892313895307/classifier/model-monitor/datacapture/classifier-xgboost-model-monitor-2020-11-08-01-54-47/AllTraffic/2020/11/08/02',\n",
       "    'LocalPath': '/opt/ml/processing/input/endpoint/classifier-xgboost-model-monitor-2020-11-08-01-54-47/AllTraffic/2020/11/08/02',\n",
       "    'S3DataType': 'S3Prefix',\n",
       "    'S3InputMode': 'File',\n",
       "    'S3DataDistributionType': 'FullyReplicated',\n",
       "    'S3CompressionType': 'None'}},\n",
       "  {'InputName': 'baseline',\n",
       "   'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-892313895307/classifier/model-monitor/baselining/results/statistics.json',\n",
       "    'LocalPath': '/opt/ml/processing/baseline/stats',\n",
       "    'S3DataType': 'S3Prefix',\n",
       "    'S3InputMode': 'File',\n",
       "    'S3DataDistributionType': 'FullyReplicated'}},\n",
       "  {'InputName': 'constraints',\n",
       "   'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-892313895307/classifier/model-monitor/baselining/results/constraints.json',\n",
       "    'LocalPath': '/opt/ml/processing/baseline/constraints',\n",
       "    'S3DataType': 'S3Prefix',\n",
       "    'S3InputMode': 'File',\n",
       "    'S3DataDistributionType': 'FullyReplicated'}}],\n",
       " 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'result',\n",
       "    'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-892313895307/classifier/model-monitor/reports/classifier-xgboost-model-monitor-2020-11-08-01-54-47/clf-xgb-model-monitor-schedule-2020-11-08-02-26-29/2020/11/08/03',\n",
       "     'LocalPath': '/opt/ml/processing/output',\n",
       "     'S3UploadMode': 'Continuous'}}]},\n",
       " 'ProcessingJobName': 'model-monitoring-202011080300-b7e786b0d056fd04f3ab1380',\n",
       " 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1,\n",
       "   'InstanceType': 'ml.r5.xlarge',\n",
       "   'VolumeSizeInGB': 20}},\n",
       " 'StoppingCondition': {'MaxRuntimeInSeconds': 3600},\n",
       " 'AppSpecification': {'ImageUri': '156813124566.dkr.ecr.us-east-1.amazonaws.com/sagemaker-model-monitor-analyzer'},\n",
       " 'Environment': {'baseline_constraints': '/opt/ml/processing/baseline/constraints/constraints.json',\n",
       "  'baseline_statistics': '/opt/ml/processing/baseline/stats/statistics.json',\n",
       "  'dataset_format': '{\"sagemakerCaptureJson\":{\"captureIndexNames\":[\"endpointInput\",\"endpointOutput\"]}}',\n",
       "  'dataset_source': '/opt/ml/processing/input/endpoint',\n",
       "  'end_time': '2020-11-08T03:00:00Z',\n",
       "  'output_path': '/opt/ml/processing/output',\n",
       "  'publish_cloudwatch_metrics': 'Enabled',\n",
       "  'sagemaker_endpoint_name': 'classifier-xgboost-model-monitor-2020-11-08-01-54-47',\n",
       "  'sagemaker_monitoring_schedule_name': 'clf-xgb-model-monitor-schedule-2020-11-08-02-26-29',\n",
       "  'start_time': '2020-11-08T02:00:00Z'},\n",
       " 'RoleArn': 'arn:aws:iam::892313895307:role/service-role/AmazonSageMaker-ExecutionRole-20200827T161464',\n",
       " 'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:892313895307:processing-job/model-monitoring-202011080300-b7e786b0d056fd04f3ab1380',\n",
       " 'ProcessingJobStatus': 'InProgress',\n",
       " 'LastModifiedTime': datetime.datetime(2020, 11, 8, 3, 6, 6, 609000, tzinfo=tzlocal()),\n",
       " 'CreationTime': datetime.datetime(2020, 11, 8, 3, 6, 6, 315000, tzinfo=tzlocal()),\n",
       " 'MonitoringScheduleArn': 'arn:aws:sagemaker:us-east-1:892313895307:monitoring-schedule/clf-xgb-model-monitor-schedule-2020-11-08-02-26-29',\n",
       " 'ResponseMetadata': {'RequestId': '8a85acd2-b663-4986-964c-15f333ccfea1',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '8a85acd2-b663-4986-964c-15f333ccfea1',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '2856',\n",
       "   'date': 'Sun, 08 Nov 2020 03:08:27 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest_execution = mon_executions[-1] # latest execution's index is -1, second to last is -2 and so on ...\n",
    "latest_execution.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest Execution Status: Completed\n",
      "Latest Execution Result: CompletedWithViolations: Job completed successfully with 1 violations.\n"
     ]
    }
   ],
   "source": [
    "print(\"Latest Execution Status: {}\".format(latest_execution.describe()['ProcessingJobStatus']))\n",
    "print(\"Latest Execution Result: {}\".format(latest_execution.describe()['ExitMessage']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report URI: s3://sagemaker-us-east-1-892313895307/classifier/model-monitor/reports/classifier-xgboost-model-monitor-2020-11-08-01-54-47/clf-xgb-model-monitor-schedule-2020-11-08-02-26-29/2020/11/08/03\n"
     ]
    }
   ],
   "source": [
    "report_uri = latest_execution.output.destination\n",
    "print('Report URI: {}'.format(report_uri))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List the Generated Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report Bucket: sagemaker-us-east-1-892313895307\n",
      "Report Key: classifier/model-monitor/reports/classifier-xgboost-model-monitor-2020-11-08-01-54-47/clf-xgb-model-monitor-schedule-2020-11-08-02-26-29/2020/11/08/03\n"
     ]
    }
   ],
   "source": [
    "s3uri = urlparse(report_uri)\n",
    "report_bucket = s3uri.netloc\n",
    "report_key = s3uri.path.lstrip('/')\n",
    "print('Report Bucket: {}'.format(report_bucket))\n",
    "print('Report Key: {}'.format(report_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Report Files:\n",
      "classifier/model-monitor/reports/classifier-xgboost-model-monitor-2020-11-08-01-54-47/clf-xgb-model-monitor-schedule-2020-11-08-02-26-29/2020/11/08/03/constraint_violations.json\n"
     ]
    }
   ],
   "source": [
    "s3_client = boto3.Session().client('s3')\n",
    "result = s3_client.list_objects(Bucket=report_bucket, Prefix=report_key)\n",
    "report_files = [report_file.get(\"Key\") for report_file in result.get('Contents')]\n",
    "print(\"Found Report Files:\")\n",
    "print(\"\\n \".join(report_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Violations Report\n",
    "\n",
    "If there are any violations compared to the baseline, they will be listed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'session' will be renamed to 'sagemaker_session' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'body_dict': {'violations': [{'feature_name': 'Extra columns',\n",
       "    'constraint_check_type': 'extra_column_check',\n",
       "    'description': 'There are extra columns in current dataset. Number of columns in current dataset: 5, Number of columns in baseline constraints: 4'}]},\n",
       " 'file_s3_uri': 's3://sagemaker-us-east-1-892313895307/classifier/model-monitor/reports/classifier-xgboost-model-monitor-2020-11-08-01-54-47/clf-xgb-model-monitor-schedule-2020-11-08-02-26-29/2020/11/08/03/constraint_violations.json',\n",
       " 'kms_key': None,\n",
       " 'session': None}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "violations = default_model_monitor.latest_monitoring_constraint_violations()\n",
    "violations.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints_df = pd.json_normalize(violations.body_dict[\"violations\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_name</th>\n",
       "      <th>constraint_check_type</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Extra columns</td>\n",
       "      <td>extra_column_check</td>\n",
       "      <td>There are extra columns in current dataset. Number of columns in current dataset: 5, Number of columns in baseline constraints: 4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    feature_name constraint_check_type  \\\n",
       "0  Extra columns  extra_column_check     \n",
       "\n",
       "                                                                                                                         description  \n",
       "0  There are extra columns in current dataset. Number of columns in current dataset: 5, Number of columns in baseline constraints: 4  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constraints_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shift in Statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "violations = default_model_monitor.latest_monitoring_constraint_violations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_shift = default_model_monitor.latest_monitoring_statistics()\n",
    "statistics_shift.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Commands\n",
    "We can also start and stop the monitoring schedules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_default_monitor.stop_monitoring_schedule()\n",
    "# my_default_monitor.start_monitoring_schedule()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the Resources\n",
    "\n",
    "You can keep your endpoint running to continue capturing data. If you do not plan to collect more data or use this endpoint further, you should delete the endpoint to avoid incurring additional charges. Note that deleting your endpoint does not delete the data that was captured during the model invocations. That data persists in Amazon S3 until you delete it yourself.\n",
    "\n",
    "But before that, you need to delete the schedule first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_default_monitor.delete_monitoring_schedule()\n",
    "# time.sleep(60) # actually wait for the deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor.delete_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
